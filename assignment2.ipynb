{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a807c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:12<00:00, 13.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "# 1. Download CIFAR-10\n",
    "# 2. Normalize to mean=0, std=1 (or use standard transform)\n",
    "# 3. Create train/test DataLoaders\n",
    "# 4. Verify data shapes (should be [batch, 3, 32, 32])\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# 1. Download CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors, scales [0,255] to [0,1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scale to [-1, 1], center at 0\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # predefined 50,000 images for training\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # predefined 10,000 images for testing\n",
    "# 3. Create train/test DataLoaders\n",
    "batch_size = 32  # Part 1 baseline requires batch_size=1 (change to 32/64 for Part 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 4. Verify data shapes\n",
    "# for images, labels in train_loader:\n",
    "#     print(f'Batch of images shape: {images.shape}')  # Should be [batch_size, 3, 32, 32] 64 images per batch, 3 channels, 32x32 pixels\n",
    "#     print(f'Batch of labels shape: {labels.shape}')  # Should be [batch_size]\n",
    "#     break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01108ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2-Layer Network to prove that cifar10 requires deeper networks.\n",
    "# **Purpose:** Prove that CIFAR-10 needs deep learning\n",
    "\n",
    "# **Architecture:**\n",
    "# ```\n",
    "# Input: 3072 (32√ó32√ó3 flattened)\n",
    "#   ‚Üì\n",
    "# Linear(3072 ‚Üí 128) + Sigmoid\n",
    "#   ‚Üì\n",
    "# Linear(128 ‚Üí 10) + Softmax\n",
    "# ```\n",
    "\n",
    "# **Implementation details:**\n",
    "# - Use `torch.nn.Linear()` (allowed)\n",
    "# - Implement sigmoid activation manually: `1 / (1 + torch.exp(-x))`\n",
    "# - Implement softmax manually: `torch.exp(x) / torch.exp(x).sum()`\n",
    "# - Implement cross-entropy loss manually\n",
    "\n",
    "# **Training setup:**\n",
    "# - Optimizer: SGD (implement manually)\n",
    "# - Batch size: 1\n",
    "# - Learning rate: 0.01 or 0.001\n",
    "# - Epochs: 10-20\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])  # Subtract max for stability\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # Compute cross-entropy loss with numerical stability\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None  # Reset gradient\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 128)  # Input: 32√ó32√ó3 = 3072\n",
    "        self.fc2 = nn.Linear(128, 10)    # Output: 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.sigmoid(self.fc1(x))  # First layer + sigmoid\n",
    "        x = self.fc2(x)  # Second layer (logits)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a27a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNet().to(device)\n",
    "learning_rate = 0.005  # Reduced from 0.01 to prevent instability\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8831ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0981, Train Acc: 26.84%\n",
      "Epoch [2/10], Loss: 1.9342, Train Acc: 32.78%\n",
      "Epoch [3/10], Loss: 1.8707, Train Acc: 35.15%\n",
      "Epoch [4/10], Loss: 1.8327, Train Acc: 36.53%\n",
      "Epoch [5/10], Loss: 1.8042, Train Acc: 37.44%\n",
      "Epoch [6/10], Loss: 1.7817, Train Acc: 38.14%\n",
      "Epoch [7/10], Loss: 1.7635, Train Acc: 38.73%\n",
      "Epoch [8/10], Loss: 1.7489, Train Acc: 39.41%\n",
      "Epoch [9/10], Loss: 1.7358, Train Acc: 39.82%\n",
      "Epoch [10/10], Loss: 1.7243, Train Acc: 40.27%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients before backward pass\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using SGD\n",
    "        model.SGD_Optimizer(model.parameters(), learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf87c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 39.93%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy in percentage\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6036c",
   "metadata": {},
   "source": [
    "# Part 2, Step 4: Activation Functions Comparison\n",
    "\n",
    "**Previous Result (Part 1):** 5-Layer CNN with Sigmoid failed completely (10% accuracy - vanishing gradients)\n",
    "\n",
    "**Purpose:** Test modern activation functions to solve vanishing gradient problem\n",
    "\n",
    "**New Activations Tested:**\n",
    "1. **Leaky ReLU** - f(x) = max(x, 0.1x)\n",
    "   - No saturation for positive values (gradient = 1)\n",
    "   - Small gradient (0.1) for negative values prevents dying neurons\n",
    "   - Used for: Conv layers 1, 2, 3\n",
    "\n",
    "2. **Tanh** - f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "   - Zero-centered output range: (-1, 1)\n",
    "   - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "   - Used for: FC layer 1\n",
    "\n",
    "**Architecture (unchanged):**\n",
    "- 3 Convolutional layers with MaxPooling (Conv: 3‚Üí16‚Üí32‚Üí64)\n",
    "- 2 Fully connected layers (FC: 1024‚Üí256‚Üí10)\n",
    "- Total: 5 parameterized layers, 288,554 parameters\n",
    "\n",
    "**Expected result:** 60-70% test accuracy (dramatic improvement from 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19197043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cuda\n",
      "Total parameters: 288554\n"
     ]
    }
   ],
   "source": [
    "# 5-Layer CNN Architecture\n",
    "class FiveLayerCNN(nn.Module):\n",
    "    def leaky_relu(self, x, negative_slope=0.1):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation: f(x) = max(x, 0.1*x)\n",
    "        - For x > 0: output = x (gradient = 1, no vanishing)\n",
    "        - For x < 0: output = 0.1*x (gradient = 0.1, prevents dying neurons)\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, negative_slope * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent: f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "        - Output range: (-1, 1)\n",
    "        - Zero-centered (better than sigmoid)\n",
    "        - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "        \"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"\n",
    "        SGD with momentum implementation following equation (8):\n",
    "        m_{i+1} = Œ± * m_i + g_i\n",
    "        Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "        \n",
    "        Args:\n",
    "            params: Model parameters\n",
    "            lr: Learning rate (Œ∑)\n",
    "            momentum: Momentum coefficient (Œ±), default=0.0 for vanilla SGD\n",
    "        \"\"\"\n",
    "        # Initialize momentum buffer on first call\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    # Get parameter id for momentum buffer\n",
    "                    param_id = id(param)\n",
    "                    \n",
    "                    # Initialize momentum to zero if not exists\n",
    "                    if param_id not in self.momentum_buffer:\n",
    "                        self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                    \n",
    "                    # Get current gradient (g_i)\n",
    "                    grad = param.grad\n",
    "                    \n",
    "                    # Update momentum: m_{i+1} = Œ± * m_i + g_i\n",
    "                    self.momentum_buffer[param_id] = momentum * self.momentum_buffer[param_id] + grad\n",
    "                    \n",
    "                    # Update parameters: Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                    param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "                    \n",
    "                    # Reset gradient\n",
    "                    param.grad = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FiveLayerCNN, self).__init__()\n",
    "        # Convolutional layers (3 parameterized layers)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # Layer 1: 3‚Üí16 channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # MaxPool (not parameterized)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Layer 2: 16‚Üí32 channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Layer 3: 32‚Üí64 channels\n",
    "        \n",
    "        # Fully connected layers (2 parameterized layers)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Layer 4: After 3 pooling ops, 32√ó32‚Üí4√ó4\n",
    "        self.fc2 = nn.Linear(256, 10)           # Layer 5: Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, 32, 32]\n",
    "        \n",
    "        # Conv block 1 - using Leaky ReLU\n",
    "        x = self.conv1(x)           # [batch, 16, 32, 32]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 16, 16, 16]\n",
    "        \n",
    "        # Conv block 2 - using Leaky ReLU\n",
    "        x = self.conv2(x)           # [batch, 32, 16, 16]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 32, 8, 8]\n",
    "        \n",
    "        # Conv block 3 - using Leaky ReLU\n",
    "        x = self.conv3(x)           # [batch, 64, 8, 8]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 64, 4, 4]\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)   # [batch, 1024]\n",
    "        \n",
    "        # Fully connected layers - using Tanh\n",
    "        x = self.fc1(x)             # [batch, 256]\n",
    "        x = self.tanh(x)            # Tanh activation\n",
    "        x = self.fc2(x)             # [batch, 10] - logits\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "cnn_model = FiveLayerCNN().to(device)\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in cnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab78ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning rate: 0.005\n",
      "  Epochs: 10\n",
      "  Batch size: 32\n",
      "  Device: cuda\n",
      "  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# Training setup for CNN\n",
    "cnn_learning_rate = 0.005  # Increased from 0.005 - Leaky ReLU and Tanh have better gradient flow\n",
    "cnn_num_epochs = 10  # More epochs for deeper network\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {cnn_learning_rate}\")\n",
    "print(f\"  Epochs: {cnn_num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1326fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2294, Train Acc: 17.74%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.9526, Train Acc: 29.70%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 1.7244, Train Acc: 38.53%, Time: 0.9min\n",
      "Epoch [4/10], Loss: 1.5282, Train Acc: 45.03%, Time: 1.1min\n",
      "Epoch [5/10], Loss: 1.4241, Train Acc: 48.70%, Time: 1.4min\n",
      "Epoch [6/10], Loss: 1.3533, Train Acc: 51.56%, Time: 1.7min\n",
      "Epoch [7/10], Loss: 1.2895, Train Acc: 53.94%, Time: 2.0min\n",
      "Epoch [8/10], Loss: 1.2289, Train Acc: 56.32%, Time: 2.3min\n",
      "Epoch [9/10], Loss: 1.1743, Train Acc: 58.22%, Time: 2.5min\n",
      "Epoch [10/10], Loss: 1.1242, Train Acc: 60.30%, Time: 2.8min\n",
      "\n",
      "Total training time: 2.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training loop for CNN\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(cnn_num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in cnn_model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = cnn_model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        cnn_model.SGD_Optimizer(cnn_model.parameters(), cnn_learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Print progress every epoch\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{cnn_num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "print(f\"\\nTotal training time: {(time.time() - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51931b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "5-Layer CNN Test Accuracy: 58.86%\n",
      "==================================================\n",
      "\n",
      "üìä Comparison:\n",
      "  2-Layer Network: 48.04%\n",
      "  5-Layer CNN:     58.86%\n",
      "  Improvement:     10.82%\n",
      "\n",
      "‚úÖ SUCCESS: Deep network achieves >50% accuracy!\n",
      "‚úÖ This proves CIFAR-10 requires deep learning!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN on test set\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_test_accuracy = 100 * correct / total\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'5-Layer CNN Test Accuracy: {cnn_test_accuracy:.2f}%')\n",
    "print(f'{\"=\"*50}')\n",
    "\n",
    "# Compare with 2-layer network\n",
    "print(f'\\nüìä Comparison:')\n",
    "print(f'  2-Layer Network: 48.04%')\n",
    "print(f'  5-Layer CNN:     {cnn_test_accuracy:.2f}%')\n",
    "print(f'  Improvement:     {cnn_test_accuracy - 48.04:.2f}%')\n",
    "\n",
    "if cnn_test_accuracy > 50:\n",
    "    print(f'\\n‚úÖ SUCCESS: Deep network achieves >{50}% accuracy!')\n",
    "    print(f'‚úÖ This proves CIFAR-10 requires deep learning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Part 1 Complete!\n",
    "\n",
    "You now have:\n",
    "1. ‚úÖ **2-Layer Network** - 48.04% test accuracy (proves shallow networks struggle)\n",
    "2. ‚úÖ **5-Layer CNN** - Should get 50-60% test accuracy (proves depth helps)\n",
    "\n",
    "### üéØ Expected Training Time:\n",
    "- With batch_size=1 and 30 epochs: **~2-3 hours**\n",
    "- Each epoch processes 50,000 images individually\n",
    "\n",
    "### üí° Tips:\n",
    "- The training will take a while - be patient!\n",
    "- Loss should steadily decrease\n",
    "- Accuracy should improve over 2-layer baseline\n",
    "- You can reduce epochs to 20 if you're short on time\n",
    "\n",
    "### üìù Next Steps (Part 2):\n",
    "After this training completes, you'll:\n",
    "1. Test different activation functions (Leaky ReLU, Tanh)\n",
    "2. Implement mini-batch SGD (batch sizes: 16, 32, 64, 128)\n",
    "3. Add momentum to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b291a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2, Step 6: Mini-Batch SGD with Momentum\n",
    "\n",
    "**Previous Results:**\n",
    "- Vanilla SGD with batch_size=1: 65.24% test accuracy (29.9 min)\n",
    "- Mini-batch SGD with batch_size=32: 59.17% test accuracy (2.8 min)\n",
    "\n",
    "**Purpose:** Add momentum to accelerate learning and smooth optimization\n",
    "\n",
    "**Momentum Algorithm (Equation 8):**\n",
    "- **m‚ÇÅ = 0** (initialize momentum to zero)\n",
    "- **g·µ¢ = (1/b) Œ£ ‚àáŒ∏·µ¢L‚Çñ** (compute gradient)\n",
    "- **m·µ¢‚Çä‚ÇÅ = Œ±¬∑m·µ¢ + g·µ¢** (update momentum with Œ± = momentum coefficient)\n",
    "- **Œ∏·µ¢‚Çä‚ÇÅ = Œ∏ - Œ∑¬∑m·µ¢‚Çä‚ÇÅ** (update parameters using momentum)\n",
    "\n",
    "**How Momentum Helps:**\n",
    "- Accumulates a moving average of gradients\n",
    "- Dampens oscillations in directions with high curvature\n",
    "- Accelerates progress along consistent descent directions\n",
    "- Think: Rolling ball gaining speed downhill\n",
    "\n",
    "**Configuration:**\n",
    "- Architecture: 5-Layer CNN with Leaky ReLU + Tanh\n",
    "- Batch size: 32 (from previous experiment)\n",
    "- Learning rate: 0.005\n",
    "- **Momentum (Œ±): Testing 3 values: 0.7, 0.9, 0.95**\n",
    "- Epochs: 10\n",
    "\n",
    "**Expected:** Faster convergence, smoother training, potentially higher accuracy than vanilla SGD. Higher Œ± should provide more smoothing but may overshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7114d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing momentum with Œ± values: [0.7, 0.9, 0.95]\n",
      "Each training will take ~2.8 minutes\n",
      "Total expected time: ~8.5 minutes\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test multiple momentum values\n",
    "import time\n",
    "\n",
    "# Test different momentum coefficients (Œ±)\n",
    "alpha_values = [0.7, 0.9, 0.95]\n",
    "momentum_results = {}\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "\n",
    "print(f\"Testing momentum with Œ± values: {alpha_values}\")\n",
    "print(f\"Each training will take ~2.8 minutes\")\n",
    "print(f\"Total expected time: ~8.5 minutes\\n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302818af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.7\n",
      "======================================================================\n",
      "\n",
      "Model created with 288554 parameters\n",
      "Configuration: lr=0.005, Œ±=0.7, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 1.9255, Train Acc: 30.18%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.4703, Train Acc: 46.94%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 1.2743, Train Acc: 54.30%, Time: 0.9min\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate for each momentum value\n",
    "for alpha in alpha_values:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîπ Training with Momentum Œ± = {alpha}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create fresh model for this alpha value\n",
    "    model_momentum = FiveLayerCNN().to(device)\n",
    "    print(f\"Model created with {sum(p.numel() for p in model_momentum.parameters())} parameters\")\n",
    "    print(f\"Configuration: lr={learning_rate}, Œ±={alpha}, epochs={num_epochs}, batch_size={batch_size}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model_momentum.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            for param in model_momentum.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model_momentum(images)\n",
    "            loss = model_momentum.cross_entropy_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights with momentum\n",
    "            model_momentum.SGD_Optimizer(model_momentum.parameters(), \n",
    "                                         learning_rate, \n",
    "                                         momentum=alpha)\n",
    "        \n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Print progress every epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining time: {training_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model_momentum.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_momentum(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Store results\n",
    "    momentum_results[alpha] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': avg_loss,\n",
    "        'training_time': training_time/60\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test Accuracy with Œ±={alpha}: {test_accuracy:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ MOMENTUM EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MOMENTUM EXPERIMENTS - COMPREHENSIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Alpha (Œ±)':<12} {'Test Acc':<12} {'Train Acc':<12} {'Loss':<12} {'Time (min)':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_alpha = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{alpha:<12.2f} {results['test_accuracy']:<12.2f} {results['train_accuracy']:<12.2f} \" \n",
    "          f\"{results['final_loss']:<12.4f} {results['training_time']:<12.1f}\")\n",
    "    \n",
    "    if results['test_accuracy'] > best_accuracy:\n",
    "        best_accuracy = results['test_accuracy']\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Momentum Value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\\n\")\n",
    "\n",
    "# Compare with baseline (mini-batch SGD without momentum)\n",
    "print(\"=\"*80)\n",
    "print(\"üìà COMPARISON WITH PREVIOUS EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Experiment':<45} {'Test Accuracy':<15} {'Training Time':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Vanilla SGD (batch_size=1)':<45} {65.24:<15.2f} {'29.9 min':<15}\")\n",
    "print(f\"{'Mini-batch SGD (batch_size=32, no momentum)':<45} {cnn_test_accuracy:<15.2f} {'2.8 min':<15}\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{f'Mini-batch SGD with Momentum (Œ±={alpha})':<45} \"\n",
    "          f\"{results['test_accuracy']:<15.2f} {results['training_time']:.1f} min\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\nüìù ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Best momentum value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\")\n",
    "print(f\"‚Ä¢ Improvement over no momentum: {best_accuracy - cnn_test_accuracy:+.2f}%\")\n",
    "print(f\"‚Ä¢ Momentum helps by accumulating gradients in consistent directions\")\n",
    "print(f\"‚Ä¢ Higher Œ± values preserve more history, lower values allow faster adaptation\")\n",
    "if best_accuracy > cnn_test_accuracy:\n",
    "    print(f\"‚Ä¢ ‚úÖ Momentum successfully improved performance!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
