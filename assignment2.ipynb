{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:00<00:00, 114MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 3.93MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:00<00:00, 56.7MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 12.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Download Fashion-MNIST\n",
    "# 2. Normalize to mean=0, std=1 (or use standard transform)\n",
    "# 3. Create train/test DataLoaders\n",
    "# 4. Verify data shapes (should be [batch, 1, 28, 28])\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# 1. Download Fashion-MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors, scales [0,255] to [0,1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1], center at 0 (grayscale: single channel)\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform) # 60,000 images for training\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform) # 10,000 images for testing\n",
    "# 3. Create train/test DataLoaders\n",
    "batch_size = 1  # Part 1 baseline requires batch_size=1 (change to 32/64 for Part 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 4. Verify data shapes\n",
    "# for images, labels in train_loader:\n",
    "#     print(f'Batch of images shape: {images.shape}')  # Should be [batch_size, 1, 28, 28] for Fashion-MNIST\n",
    "#     print(f'Batch of labels shape: {labels.shape}')  # Should be [batch_size]\n",
    "#     break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01108ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2-Layer Network to prove that Fashion-MNIST requires deeper networks.\n",
    "# **Purpose:** Prove that Fashion-MNIST needs deep learning\n",
    "\n",
    "# **Architecture:**\n",
    "# ```\n",
    "# Input: 784 (28√ó28√ó1 flattened)\n",
    "#   ‚Üì\n",
    "# Linear(784 ‚Üí 256) + ReLU\n",
    "#   ‚Üì\n",
    "# Linear(256 ‚Üí 10) + Softmax\n",
    "# ```\n",
    "\n",
    "# **Implementation details:**\n",
    "# - Use `torch.nn.Linear()` (allowed)\n",
    "# - Implement sigmoid activation manually: `1 / (1 + torch.exp(-x))`\n",
    "# - Implement softmax manually: `torch.exp(x) / torch.exp(x).sum()`\n",
    "# - Implement cross-entropy loss manually\n",
    "\n",
    "# **Training setup:**\n",
    "# - Optimizer: SGD (implement manually)\n",
    "# - Batch size: 1\n",
    "# - Learning rate: 0.01 or 0.001\n",
    "# - Epochs: 10-20\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])  # Subtract max for stability\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # Compute cross-entropy loss with numerical stability\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None  # Reset gradient\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)   # Input: 28√ó28√ó1 = 784 (Fashion-MNIST)\n",
    "        self.fc2 = nn.Linear(256, 10)    # Output: 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.sigmoid(self.fc1(x))  # First layer + sigmoid\n",
    "        x = self.fc2(x)  # Second layer (logits)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a27a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNet().to(device)\n",
    "learning_rate = 0.005  # Reduced from 0.01 to prevent instability\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8831ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4423, Train Acc: 63.03%\n",
      "Epoch [2/10], Loss: 0.8364, Train Acc: 73.64%\n",
      "Epoch [3/10], Loss: 0.6978, Train Acc: 75.96%\n",
      "Epoch [4/10], Loss: 0.6341, Train Acc: 77.65%\n",
      "Epoch [5/10], Loss: 0.5933, Train Acc: 79.06%\n",
      "Epoch [6/10], Loss: 0.5631, Train Acc: 80.16%\n",
      "Epoch [7/10], Loss: 0.5405, Train Acc: 81.08%\n",
      "Epoch [8/10], Loss: 0.5227, Train Acc: 81.70%\n",
      "Epoch [9/10], Loss: 0.5083, Train Acc: 82.29%\n",
      "Epoch [10/10], Loss: 0.4963, Train Acc: 82.61%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients before backward pass\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using SGD\n",
    "        model.SGD_Optimizer(model.parameters(), learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf87c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.37%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy in percentage\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6036c",
   "metadata": {},
   "source": [
    "# Part 2, Step 4: Activation Functions Comparison\n",
    "\n",
    "**Previous Result (Part 1):** 5-Layer CNN with Sigmoid failed completely (10% accuracy - vanishing gradients)\n",
    "\n",
    "**Purpose:** Test modern activation functions to solve vanishing gradient problem\n",
    "\n",
    "**New Activations Tested:**\n",
    "1. **Leaky ReLU** - f(x) = max(x, 0.1x)\n",
    "   - No saturation for positive values (gradient = 1)\n",
    "   - Small gradient (0.1) for negative values prevents dying neurons\n",
    "   - Used for: Conv layers 1, 2, 3\n",
    "\n",
    "2. **Tanh** - f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "   - Zero-centered output range: (-1, 1)\n",
    "   - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "   - Used for: FC layer 1\n",
    "\n",
    "**Architecture (unchanged):**\n",
    "- 3 Convolutional layers with MaxPooling (Conv: 3‚Üí16‚Üí32‚Üí64)\n",
    "- 2 Fully connected layers (FC: 1024‚Üí256‚Üí10)\n",
    "- Total: 5 parameterized layers, 288,554 parameters\n",
    "\n",
    "**Expected result:** 60-70% test accuracy (dramatic improvement from 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21789a",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ MISSING EXPERIMENT 1: Activation Function Comparison (batch_size=1)\n",
    "\n",
    "**Purpose:** Demonstrate the vanishing gradient problem with sigmoid vs modern activations\n",
    "\n",
    "We'll train TWO 5-layer CNNs with **batch_size=1** to compare:\n",
    "1. **Sigmoid activation** - Expected to fail due to vanishing gradients (~10-20% accuracy)\n",
    "2. **Leaky ReLU + Tanh** - Expected to succeed (~75-85% accuracy)\n",
    "\n",
    "This comparison demonstrates why modern activation functions are essential for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68156db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT: Activation Function Comparison (batch_size=1)\n",
      "======================================================================\n",
      "DataLoaders created with batch_size=1\n",
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set batch_size=1 for activation function comparison\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT: Activation Function Comparison (batch_size=1)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataLoaders with batch_size=1\n",
    "train_loader_bs1 = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader_bs1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created with batch_size=1\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de60ff8",
   "metadata": {},
   "source": [
    "### Experiment 1a: 5-Layer CNN with SIGMOID (Vanishing Gradient Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25a6e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 5-Layer CNN with SIGMOID created\n",
      "   Parameters: 173578\n"
     ]
    }
   ],
   "source": [
    "# 5-Layer CNN with SIGMOID activation (to demonstrate vanishing gradients)\n",
    "class FiveLayerCNN_Sigmoid(nn.Module):\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation: œÉ(x) = 1/(1+e^(-x))\n",
    "        - Output range: (0, 1)\n",
    "        - Max gradient: 0.25 (causes vanishing gradients!)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FiveLayerCNN_Sigmoid, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # All layers use sigmoid activation\n",
    "        x = self.sigmoid(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.sigmoid(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.sigmoid(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create sigmoid model\n",
    "model_sigmoid = FiveLayerCNN_Sigmoid().to(device)\n",
    "print(\"‚úÖ 5-Layer CNN with SIGMOID created\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_sigmoid.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING: 5-Layer CNN with SIGMOID (batch_size=1)\n",
      "======================================================================\n",
      "‚ö†Ô∏è  WARNING: This should fail due to vanishing gradients!\n",
      "   Expected accuracy: 10-20% (near random guessing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train sigmoid model (batch_size=1)\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING: 5-Layer CNN with SIGMOID (batch_size=1)\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  WARNING: This should fail due to vanishing gradients!\")\n",
    "print(\"   Expected accuracy: 10-20% (near random guessing)\\n\")\n",
    "\n",
    "learning_rate_sigmoid = 0.005\n",
    "num_epochs_sigmoid = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs_sigmoid):\n",
    "    model_sigmoid.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader_bs1:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        for param in model_sigmoid.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        outputs = model_sigmoid(images)\n",
    "        loss = model_sigmoid.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        model_sigmoid.SGD_Optimizer(model_sigmoid.parameters(), learning_rate_sigmoid)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader_bs1)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_sigmoid}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed/60:.1f}min')\n",
    "\n",
    "training_time_sigmoid = time.time() - start_time\n",
    "print(f\"\\nTraining time: {training_time_sigmoid/60:.1f} minutes\")\n",
    "\n",
    "# Evaluate sigmoid model\n",
    "model_sigmoid.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_bs1:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_sigmoid(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_sigmoid = 100 * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS: 5-Layer CNN with SIGMOID\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {accuracy_sigmoid:.2f}%\")\n",
    "print(f\"Training Time: {training_time_sigmoid/60:.1f} minutes\")\n",
    "\n",
    "if accuracy_sigmoid < 30:\n",
    "    print(\"\\n‚ùå VANISHING GRADIENT PROBLEM DEMONSTRATED!\")\n",
    "    print(\"   Sigmoid activations prevent effective learning in deep networks\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Unexpected: Sigmoid performed better than expected\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3e29c",
   "metadata": {},
   "source": [
    "### Experiment 1b: 5-Layer CNN with LEAKY ReLU + TANH (batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897006c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Leaky ReLU + Tanh model for batch_size=1 comparison\n",
    "model_leakyrelu_bs1 = FiveLayerCNN().to(device)\n",
    "print(\"\\n‚úÖ 5-Layer CNN with LEAKY ReLU + TANH created\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_leakyrelu_bs1.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Leaky ReLU model (batch_size=1)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING: 5-Layer CNN with LEAKY ReLU + TANH (batch_size=1)\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ This should succeed with modern activation functions!\")\n",
    "print(\"   Expected accuracy: 75-85%\\n\")\n",
    "\n",
    "learning_rate_leaky = 0.005\n",
    "num_epochs_leaky = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs_leaky):\n",
    "    model_leakyrelu_bs1.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader_bs1:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        for param in model_leakyrelu_bs1.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        outputs = model_leakyrelu_bs1(images)\n",
    "        loss = model_leakyrelu_bs1.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        model_leakyrelu_bs1.SGD_Optimizer(model_leakyrelu_bs1.parameters(), learning_rate_leaky)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader_bs1)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_leaky}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed/60:.1f}min')\n",
    "\n",
    "training_time_leaky = time.time() - start_time\n",
    "print(f\"\\nTraining time: {training_time_leaky/60:.1f} minutes\")\n",
    "\n",
    "# Evaluate Leaky ReLU model\n",
    "model_leakyrelu_bs1.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_bs1:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_leakyrelu_bs1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_leaky = 100 * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS: 5-Layer CNN with LEAKY ReLU + TANH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {accuracy_leaky:.2f}%\")\n",
    "print(f\"Training Time: {training_time_leaky/60:.1f} minutes\")\n",
    "\n",
    "if accuracy_leaky > accuracy_sigmoid + 20:\n",
    "    print(\"\\n‚úÖ SUCCESS! Modern activations dramatically outperform sigmoid\")\n",
    "    print(f\"   Improvement: +{accuracy_leaky - accuracy_sigmoid:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Unexpected: Smaller improvement than expected\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ACTIVATION FUNCTION COMPARISON SUMMARY (batch_size=1)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<40} {'Test Acc':<12} {'Time':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'5-Layer CNN with SIGMOID':<40} {accuracy_sigmoid:<12.2f} {training_time_sigmoid/60:.1f} min\")\n",
    "print(f\"{'5-Layer CNN with LEAKY ReLU + TANH':<40} {accuracy_leaky:<12.2f} {training_time_leaky/60:.1f} min\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüí° Key Insight:\")\n",
    "print(f\"   Modern activation functions (Leaky ReLU, Tanh) enable deep learning\")\n",
    "print(f\"   Sigmoid causes vanishing gradients in deep networks\")\n",
    "print(f\"   Improvement: {accuracy_leaky - accuracy_sigmoid:+.2f} percentage points\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb77c6",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Activation Function Experiment Complete!\n",
    "\n",
    "Now continuing with batch_size=32 experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to batch_size=32 for remaining experiments\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Switched to batch_size=32 for remaining experiments\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19197043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cuda\n",
      "Total parameters: 173578\n"
     ]
    }
   ],
   "source": [
    "# 5-Layer CNN Architecture\n",
    "class FiveLayerCNN(nn.Module):\n",
    "    def leaky_relu(self, x, negative_slope=0.1):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation: f(x) = max(x, 0.1*x)\n",
    "        - For x > 0: output = x (gradient = 1, no vanishing)\n",
    "        - For x < 0: output = 0.1*x (gradient = 0.1, prevents dying neurons)\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, negative_slope * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent: f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "        - Output range: (-1, 1)\n",
    "        - Zero-centered (better than sigmoid)\n",
    "        - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "        \"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"\n",
    "        SGD with momentum implementation following equation (8):\n",
    "        m_{i+1} = Œ± * m_i + g_i\n",
    "        Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "        \n",
    "        Args:\n",
    "            params: Model parameters\n",
    "            lr: Learning rate (Œ∑)\n",
    "            momentum: Momentum coefficient (Œ±), default=0.0 for vanilla SGD\n",
    "        \"\"\"\n",
    "        # Initialize momentum buffer on first call\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    # Get parameter id for momentum buffer\n",
    "                    param_id = id(param)\n",
    "                    \n",
    "                    # Initialize momentum to zero if not exists\n",
    "                    if param_id not in self.momentum_buffer:\n",
    "                        self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                    \n",
    "                    # Get current gradient (g_i)\n",
    "                    grad = param.grad\n",
    "                    \n",
    "                    # Update momentum: m_{i+1} = Œ± * m_i + g_i\n",
    "                    self.momentum_buffer[param_id] = momentum * self.momentum_buffer[param_id] + grad\n",
    "                    \n",
    "                    # Update parameters: Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                    param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "                    \n",
    "                    # Reset gradient\n",
    "                    param.grad = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FiveLayerCNN, self).__init__()\n",
    "        # Convolutional layers (3 parameterized layers)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # Layer 1: 1‚Üí16 channels (grayscale)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # MaxPool (not parameterized)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Layer 2: 16‚Üí32 channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Layer 3: 32‚Üí64 channels\n",
    "        \n",
    "        # Fully connected layers (2 parameterized layers)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)  # Layer 4: After 3 pooling ops, 28√ó28‚Üí14√ó14‚Üí7√ó7‚Üí3√ó3\n",
    "        self.fc2 = nn.Linear(256, 10)           # Layer 5: Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 1, 28, 28] for Fashion-MNIST\n",
    "        \n",
    "        # Conv block 1 - using Leaky ReLU\n",
    "        x = self.conv1(x)           # [batch, 16, 28, 28]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 16, 14, 14]\n",
    "        \n",
    "        # Conv block 2 - using Leaky ReLU\n",
    "        x = self.conv2(x)           # [batch, 32, 14, 14]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 32, 7, 7]\n",
    "        \n",
    "        # Conv block 3 - using Leaky ReLU\n",
    "        x = self.conv3(x)           # [batch, 64, 7, 7]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 64, 3, 3]\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)   # [batch, 576]\n",
    "        \n",
    "        # Fully connected layers - using Tanh\n",
    "        x = self.fc1(x)             # [batch, 256]\n",
    "        x = self.tanh(x)            # Tanh activation\n",
    "        x = self.fc2(x)             # [batch, 10] - logits\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "cnn_model = FiveLayerCNN().to(device)\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in cnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab78ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning rate: 0.005\n",
      "  Epochs: 10\n",
      "  Batch size: 32\n",
      "  Device: cuda\n",
      "  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# Training setup for CNN\n",
    "cnn_learning_rate = 0.005  # Increased from 0.005 - Leaky ReLU and Tanh have better gradient flow\n",
    "cnn_num_epochs = 10  # More epochs for deeper network\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {cnn_learning_rate}\")\n",
    "print(f\"  Epochs: {cnn_num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1326fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4667, Train Acc: 51.77%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 0.6559, Train Acc: 75.72%, Time: 0.7min\n",
      "Epoch [3/10], Loss: 0.5499, Train Acc: 79.56%, Time: 1.0min\n",
      "Epoch [4/10], Loss: 0.4942, Train Acc: 81.75%, Time: 1.3min\n",
      "Epoch [5/10], Loss: 0.4532, Train Acc: 83.31%, Time: 1.7min\n",
      "Epoch [6/10], Loss: 0.4210, Train Acc: 84.48%, Time: 2.0min\n",
      "Epoch [7/10], Loss: 0.3961, Train Acc: 85.42%, Time: 2.3min\n",
      "Epoch [8/10], Loss: 0.3763, Train Acc: 86.25%, Time: 2.7min\n",
      "Epoch [9/10], Loss: 0.3593, Train Acc: 86.74%, Time: 3.0min\n",
      "Epoch [10/10], Loss: 0.3444, Train Acc: 87.26%, Time: 3.3min\n",
      "\n",
      "Total training time: 3.3 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training loop for CNN\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(cnn_num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in cnn_model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = cnn_model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        cnn_model.SGD_Optimizer(cnn_model.parameters(), cnn_learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Print progress every epoch\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{cnn_num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "print(f\"\\nTotal training time: {(time.time() - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51931b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "5-Layer CNN Test Accuracy: 85.57%\n",
      "==================================================\n",
      "\n",
      "üìä Comparison:\n",
      "  2-Layer Network: 48.04%\n",
      "  5-Layer CNN:     85.57%\n",
      "  Improvement:     37.53%\n",
      "\n",
      "‚úÖ SUCCESS: Deep network achieves >50% accuracy!\n",
      "‚úÖ This proves CIFAR-10 requires deep learning!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN on test set\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_test_accuracy = 100 * correct / total\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'5-Layer CNN Test Accuracy: {cnn_test_accuracy:.2f}%')\n",
    "print(f'{\"=\"*50}')\n",
    "\n",
    "# Compare with 2-layer network\n",
    "print(f'\\nüìä Comparison:')\n",
    "print(f'  2-Layer Network: 48.04%')\n",
    "print(f'  5-Layer CNN:     {cnn_test_accuracy:.2f}%')\n",
    "print(f'  Improvement:     {cnn_test_accuracy - 48.04:.2f}%')\n",
    "\n",
    "if cnn_test_accuracy > 50:\n",
    "    print(f'\\n‚úÖ SUCCESS: Deep network achieves >{50}% accuracy!')\n",
    "    print(f'‚úÖ This proves CIFAR-10 requires deep learning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Part 1 Complete!\n",
    "\n",
    "You now have:\n",
    "1. ‚úÖ **2-Layer Network** - 48.04% test accuracy (proves shallow networks struggle)\n",
    "2. ‚úÖ **5-Layer CNN** - Should get 50-60% test accuracy (proves depth helps)\n",
    "\n",
    "### üéØ Expected Training Time:\n",
    "- With batch_size=1 and 30 epochs: **~2-3 hours**\n",
    "- Each epoch processes 50,000 images individually\n",
    "\n",
    "### üí° Tips:\n",
    "- The training will take a while - be patient!\n",
    "- Loss should steadily decrease\n",
    "- Accuracy should improve over 2-layer baseline\n",
    "- You can reduce epochs to 20 if you're short on time\n",
    "\n",
    "### üìù Next Steps (Part 2):\n",
    "After this training completes, you'll:\n",
    "1. Test different activation functions (Leaky ReLU, Tanh)\n",
    "2. Implement mini-batch SGD (batch sizes: 16, 32, 64, 128)\n",
    "3. Add momentum to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b291a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2, Step 6: Mini-Batch SGD with Momentum\n",
    "\n",
    "**Previous Results:**\n",
    "- Vanilla SGD with batch_size=1: 65.24% test accuracy (29.9 min)\n",
    "- Mini-batch SGD with batch_size=32: 59.17% test accuracy (2.8 min)\n",
    "\n",
    "**Purpose:** Add momentum to accelerate learning and smooth optimization\n",
    "\n",
    "**Momentum Algorithm (Equation 8):**\n",
    "- **m‚ÇÅ = 0** (initialize momentum to zero)\n",
    "- **g·µ¢ = (1/b) Œ£ ‚àáŒ∏·µ¢L‚Çñ** (compute gradient)\n",
    "- **m·µ¢‚Çä‚ÇÅ = Œ±¬∑m·µ¢ + g·µ¢** (update momentum with Œ± = momentum coefficient)\n",
    "- **Œ∏·µ¢‚Çä‚ÇÅ = Œ∏ - Œ∑¬∑m·µ¢‚Çä‚ÇÅ** (update parameters using momentum)\n",
    "\n",
    "**How Momentum Helps:**\n",
    "- Accumulates a moving average of gradients\n",
    "- Dampens oscillations in directions with high curvature\n",
    "- Accelerates progress along consistent descent directions\n",
    "- Think: Rolling ball gaining speed downhill\n",
    "\n",
    "**Configuration:**\n",
    "- Architecture: 5-Layer CNN with Leaky ReLU + Tanh\n",
    "- Batch size: 32 (from previous experiment)\n",
    "- Learning rate: 0.005\n",
    "- **Momentum (Œ±): Testing 3 values: 0.7, 0.9, 0.95**\n",
    "- Epochs: 10\n",
    "\n",
    "**Expected:** Faster convergence, smoother training, potentially higher accuracy than vanilla SGD. Higher Œ± should provide more smoothing but may overshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing momentum with Œ± values: [0.7, 0.9, 0.95]\n",
      "Each training will take ~2.8 minutes\n",
      "Total expected time: ~8.5 minutes\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test multiple momentum values\n",
    "import time\n",
    "\n",
    "# Test different momentum coefficients (Œ±) - UPDATED to include 0.5 and 0.99\n",
    "alpha_values = [0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "momentum_results = {}\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "\n",
    "print(f\"Testing momentum with Œ± values: {alpha_values}\")\n",
    "print(f\"Each training will take ~3.3 minutes\")\n",
    "print(f\"Total expected time: ~16.5 minutes\\n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302818af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.7\n",
      "======================================================================\n",
      "\n",
      "Model created with 173578 parameters\n",
      "Configuration: lr=0.005, Œ±=0.7, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 0.8720, Train Acc: 68.57%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 0.4657, Train Acc: 82.77%, Time: 0.7min\n",
      "Epoch [3/10], Loss: 0.3827, Train Acc: 85.98%, Time: 1.0min\n",
      "Epoch [4/10], Loss: 0.3388, Train Acc: 87.42%, Time: 1.3min\n",
      "Epoch [5/10], Loss: 0.3137, Train Acc: 88.42%, Time: 1.7min\n",
      "Epoch [6/10], Loss: 0.2929, Train Acc: 89.11%, Time: 2.0min\n",
      "Epoch [7/10], Loss: 0.2779, Train Acc: 89.58%, Time: 2.3min\n",
      "Epoch [8/10], Loss: 0.2643, Train Acc: 90.26%, Time: 2.6min\n",
      "Epoch [9/10], Loss: 0.2512, Train Acc: 90.65%, Time: 3.0min\n",
      "Epoch [10/10], Loss: 0.2410, Train Acc: 91.00%, Time: 3.3min\n",
      "\n",
      "Training time: 3.3 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.7: 89.39%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.9\n",
      "======================================================================\n",
      "\n",
      "Model created with 173578 parameters\n",
      "Configuration: lr=0.005, Œ±=0.9, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 0.6181, Train Acc: 77.20%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 0.3482, Train Acc: 87.12%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 0.2969, Train Acc: 89.02%, Time: 1.0min\n",
      "Epoch [4/10], Loss: 0.2669, Train Acc: 90.10%, Time: 1.3min\n",
      "Epoch [5/10], Loss: 0.2452, Train Acc: 90.95%, Time: 1.6min\n",
      "Epoch [6/10], Loss: 0.2240, Train Acc: 91.63%, Time: 2.0min\n",
      "Epoch [7/10], Loss: 0.2103, Train Acc: 92.25%, Time: 2.3min\n",
      "Epoch [8/10], Loss: 0.1949, Train Acc: 92.74%, Time: 2.6min\n",
      "Epoch [9/10], Loss: 0.1806, Train Acc: 93.15%, Time: 3.0min\n",
      "Epoch [10/10], Loss: 0.1698, Train Acc: 93.61%, Time: 3.3min\n",
      "\n",
      "Training time: 3.3 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.9: 90.91%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.95\n",
      "======================================================================\n",
      "\n",
      "Model created with 173578 parameters\n",
      "Configuration: lr=0.005, Œ±=0.95, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 0.5412, Train Acc: 80.22%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 0.3135, Train Acc: 88.38%, Time: 0.7min\n",
      "Epoch [3/10], Loss: 0.2706, Train Acc: 89.93%, Time: 1.0min\n",
      "Epoch [4/10], Loss: 0.2420, Train Acc: 91.04%, Time: 1.3min\n",
      "Epoch [5/10], Loss: 0.2214, Train Acc: 91.69%, Time: 1.7min\n",
      "Epoch [6/10], Loss: 0.2031, Train Acc: 92.48%, Time: 2.0min\n",
      "Epoch [7/10], Loss: 0.1904, Train Acc: 92.78%, Time: 2.3min\n",
      "Epoch [8/10], Loss: 0.1742, Train Acc: 93.47%, Time: 2.6min\n",
      "Epoch [9/10], Loss: 0.1607, Train Acc: 93.91%, Time: 3.0min\n",
      "Epoch [10/10], Loss: 0.1516, Train Acc: 94.17%, Time: 3.3min\n",
      "\n",
      "Training time: 3.3 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.95: 91.05%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéØ MOMENTUM EXPERIMENTS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate for each momentum value\n",
    "for alpha in alpha_values:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîπ Training with Momentum Œ± = {alpha}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create fresh model for this alpha value\n",
    "    model_momentum = FiveLayerCNN().to(device)\n",
    "    print(f\"Model created with {sum(p.numel() for p in model_momentum.parameters())} parameters\")\n",
    "    print(f\"Configuration: lr={learning_rate}, Œ±={alpha}, epochs={num_epochs}, batch_size={batch_size}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model_momentum.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            for param in model_momentum.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model_momentum(images)\n",
    "            loss = model_momentum.cross_entropy_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights with momentum\n",
    "            model_momentum.SGD_Optimizer(model_momentum.parameters(), \n",
    "                                         learning_rate, \n",
    "                                         momentum=alpha)\n",
    "        \n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Print progress every epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining time: {training_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model_momentum.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_momentum(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Store results\n",
    "    momentum_results[alpha] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': avg_loss,\n",
    "        'training_time': training_time/60\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test Accuracy with Œ±={alpha}: {test_accuracy:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ MOMENTUM EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0b0776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä MOMENTUM EXPERIMENTS - COMPREHENSIVE RESULTS\n",
      "================================================================================\n",
      "Alpha (Œ±)    Test Acc     Train Acc    Loss         Time (min)  \n",
      "--------------------------------------------------------------------------------\n",
      "0.70         89.39        91.00        0.2410       3.3         \n",
      "0.90         90.91        93.61        0.1698       3.3         \n",
      "0.95         91.05        94.17        0.1516       3.3         \n",
      "================================================================================\n",
      "\n",
      "üèÜ Best Momentum Value: Œ± = 0.95 with 91.05% test accuracy\n",
      "\n",
      "================================================================================\n",
      "üìà COMPARISON WITH PREVIOUS EXPERIMENTS\n",
      "================================================================================\n",
      "Experiment                                    Test Accuracy   Training Time  \n",
      "--------------------------------------------------------------------------------\n",
      "Vanilla SGD (batch_size=1)                    65.24           29.9 min       \n",
      "Mini-batch SGD (batch_size=32, no momentum)   85.57           2.8 min        \n",
      "Mini-batch SGD with Momentum (Œ±=0.7)          89.39           3.3 min\n",
      "Mini-batch SGD with Momentum (Œ±=0.9)          90.91           3.3 min\n",
      "Mini-batch SGD with Momentum (Œ±=0.95)         91.05           3.3 min\n",
      "================================================================================\n",
      "\n",
      "üìù ANALYSIS:\n",
      "‚Ä¢ Best momentum value: Œ± = 0.95 with 91.05% test accuracy\n",
      "‚Ä¢ Improvement over no momentum: +5.48%\n",
      "‚Ä¢ Momentum helps by accumulating gradients in consistent directions\n",
      "‚Ä¢ Higher Œ± values preserve more history, lower values allow faster adaptation\n",
      "‚Ä¢ ‚úÖ Momentum successfully improved performance!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive results comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MOMENTUM EXPERIMENTS - COMPREHENSIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Alpha (Œ±)':<12} {'Test Acc':<12} {'Train Acc':<12} {'Loss':<12} {'Time (min)':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_alpha = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{alpha:<12.2f} {results['test_accuracy']:<12.2f} {results['train_accuracy']:<12.2f} \" \n",
    "          f\"{results['final_loss']:<12.4f} {results['training_time']:<12.1f}\")\n",
    "    \n",
    "    if results['test_accuracy'] > best_accuracy:\n",
    "        best_accuracy = results['test_accuracy']\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Momentum Value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\\n\")\n",
    "\n",
    "# Compare with baseline (mini-batch SGD without momentum)\n",
    "print(\"=\"*80)\n",
    "print(\"üìà COMPARISON WITH PREVIOUS EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Experiment':<45} {'Test Accuracy':<15} {'Training Time':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Vanilla SGD (batch_size=1)':<45} {65.24:<15.2f} {'29.9 min':<15}\")\n",
    "print(f\"{'Mini-batch SGD (batch_size=32, no momentum)':<45} {cnn_test_accuracy:<15.2f} {'2.8 min':<15}\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{f'Mini-batch SGD with Momentum (Œ±={alpha})':<45} \"\n",
    "          f\"{results['test_accuracy']:<15.2f} {results['training_time']:.1f} min\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\nüìù ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Best momentum value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\")\n",
    "print(f\"‚Ä¢ Improvement over no momentum: {best_accuracy - cnn_test_accuracy:+.2f}%\")\n",
    "print(f\"‚Ä¢ Momentum helps by accumulating gradients in consistent directions\")\n",
    "print(f\"‚Ä¢ Higher Œ± values preserve more history, lower values allow faster adaptation\")\n",
    "if best_accuracy > cnn_test_accuracy:\n",
    "    print(f\"‚Ä¢ ‚úÖ Momentum successfully improved performance!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2037e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: SKIP CONNECTIONS (Residual Learning)\n",
    "\n",
    "## Motivation\n",
    "As networks become deeper, training becomes more difficult due to:\n",
    "1. **Vanishing gradients**: Gradients become exponentially small in early layers\n",
    "2. **Degradation problem**: Deep networks can perform worse than shallow ones (not due to overfitting)\n",
    "3. **Optimization difficulty**: Deeper networks are harder to optimize\n",
    "\n",
    "**Skip connections (Residual connections)** solve this by:\n",
    "- Creating shortcuts that bypass layers\n",
    "- Allowing gradients to flow directly backward\n",
    "- Learning residual functions F(x) = H(x) - x instead of H(x) directly\n",
    "- Enabling very deep networks (50, 100, even 1000+ layers)\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "We will conduct **THREE experiments** with a 15-layer deep CNN:\n",
    "1. **Extended model WITHOUT skip connections** (plain deep network) - Baseline to observe degradation\n",
    "2. **Extended model WITH skip connections - Configuration 1** - First skip connection strategy (3 skips)\n",
    "3. **Extended model WITH skip connections - Configuration 2** - Second skip connection strategy (3 skips)\n",
    "\n",
    "**Assignment Requirement:** Add 10 layers to our 5-layer CNN (making it 15 layers total), then test with two different skip connection configurations, each using 3 skip connections.\n",
    "\n",
    "**Hypothesis:** \n",
    "- The plain 15-layer network may suffer from degradation (perform worse than 5-layer)\n",
    "- Skip connections will enable effective training and improve gradient flow\n",
    "- Different skip configurations may yield different performance\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "**15-Layer CNN Structure (Extended Model):**\n",
    "- **Block 1:** Conv 3‚Üí16, then 3√ó Conv 16‚Üí16 (same shape, ideal for skip connections)\n",
    "- **Block 2:** Conv 16‚Üí32, then 3√ó Conv 32‚Üí32 (same shape, ideal for skip connections)\n",
    "- **Block 3:** Conv 32‚Üí64, then 3√ó Conv 64‚Üí64 (same shape, ideal for skip connections)\n",
    "- **Block 4:** FC 1024‚Üí256, FC 256‚Üí256, FC 256‚Üí10\n",
    "- **Total:** 15 parameterized layers (12 conv + 3 FC) = 5-layer baseline + 10 additional layers\n",
    "\n",
    "### Skip Connection Configurations\n",
    "\n",
    "**Configuration 1: Short Skips (Length 1)** - ResNet-style, frequent gradient shortcuts\n",
    "- Skip 1: conv1 output ‚Üí conv2 output (y = x ‚äï f(x), skips 1 layer)\n",
    "- Skip 2: conv5 output ‚Üí conv6 output (y = x ‚äï f(x), skips 1 layer) \n",
    "- Skip 3: conv9 output ‚Üí conv10 output (y = x ‚äï f(x), skips 1 layer)\n",
    "\n",
    "**Configuration 2: Longer Skips (Length 2-3)** - Deeper gradient shortcuts\n",
    "- Skip 1: conv1 output ‚Üí conv3 output (y = x ‚äï g(f(x)), skips 2 layers)\n",
    "- Skip 2: conv5 output ‚Üí conv8 output (y = x ‚äï h(g(f(x))), skips 3 layers)\n",
    "- Skip 3: conv9 output ‚Üí conv12 output (y = x ‚äï h(g(f(x))), skips 3 layers)\n",
    "\n",
    "**Training Configuration:**\n",
    "- Use best hyperparameters from Part 2: batch_size=32, momentum=0.9, lr=0.005\n",
    "- Train for 10 epochs for all three models\n",
    "- Record average L1-norm of gradients during first epoch for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "471732e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "15-LAYER DEEP CNN (NO SKIP CONNECTIONS)\n",
      "======================================================================\n",
      "Total parameters: 384,858\n",
      "\n",
      "Architecture:\n",
      "  Block 1: Conv 3‚Üí16, 3√ó Conv 16‚Üí16, MaxPool\n",
      "  Block 2: Conv 16‚Üí32, 3√ó Conv 32‚Üí32, MaxPool\n",
      "  Block 3: Conv 32‚Üí64, 3√ó Conv 64‚Üí64, MaxPool\n",
      "  Block 4: FC 1024‚Üí256, FC 256‚Üí256, FC 256‚Üí10\n",
      "\n",
      "Total: 15 parameterized layers (12 conv + 3 FC)\n",
      "Activation: Leaky ReLU (conv), Tanh (FC)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 15-Layer Deep CNN WITHOUT Skip Connections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_NoSkip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_NoSkip, self).__init__()\n",
    "        \n",
    "        # Block 1: 1‚Üí16‚Üí16‚Üí16‚Üí16 (grayscale input)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 576‚Üí256‚Üí256‚Üí10 (28√ó28‚Üí14√ó14‚Üí7√ó7‚Üí3√ó3)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv5(x))\n",
    "        x = self.leaky_relu(self.conv6(x))\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv9(x))\n",
    "        x = self.leaky_relu(self.conv10(x))\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test the model\n",
    "model_deep15_no_skip = DeepCNN15_NoSkip().to(device)\n",
    "total_params = sum(p.numel() for p in model_deep15_no_skip.parameters())\n",
    "print(\"=\"*70)\n",
    "print(\"15-LAYER DEEP CNN (NO SKIP CONNECTIONS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  Block 1: Conv 3‚Üí16, 3√ó Conv 16‚Üí16, MaxPool\")\n",
    "print(\"  Block 2: Conv 16‚Üí32, 3√ó Conv 32‚Üí32, MaxPool\")\n",
    "print(\"  Block 3: Conv 32‚Üí64, 3√ó Conv 64‚Üí64, MaxPool\")\n",
    "print(\"  Block 4: FC 1024‚Üí256, FC 256‚Üí256, FC 256‚Üí10\")\n",
    "print(\"\\nTotal: 15 parameterized layers (12 conv + 3 FC)\")\n",
    "print(\"Activation: Leaky ReLU (conv), Tanh (FC)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ff5ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING 15-LAYER CNN (NO SKIP CONNECTIONS)\n",
      "======================================================================\n",
      "Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\n",
      "Recording gradient L1-norms during first epoch...\n",
      "\n",
      "Epoch [1/10], Loss: 2.3032, Train Acc: 9.92%, Time: 0.6min\n",
      "Epoch [2/10], Loss: 2.3034, Train Acc: 9.91%, Time: 1.0min\n",
      "Epoch [3/10], Loss: 2.3033, Train Acc: 9.87%, Time: 1.5min\n",
      "Epoch [4/10], Loss: 2.3033, Train Acc: 10.12%, Time: 2.0min\n",
      "Epoch [5/10], Loss: 2.3033, Train Acc: 9.83%, Time: 2.5min\n",
      "Epoch [6/10], Loss: 2.3031, Train Acc: 10.05%, Time: 3.0min\n",
      "Epoch [7/10], Loss: 2.3032, Train Acc: 10.04%, Time: 3.4min\n",
      "Epoch [8/10], Loss: 2.3032, Train Acc: 9.86%, Time: 3.9min\n",
      "Epoch [9/10], Loss: 2.3033, Train Acc: 9.93%, Time: 4.4min\n",
      "Epoch [10/10], Loss: 2.3032, Train Acc: 9.76%, Time: 4.9min\n",
      "\n",
      "‚úÖ Training complete!\n",
      "Training time: 4.9 minutes\n",
      "Average gradient L1-norm (epoch 1): 18.19\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train 15-layer CNN WITHOUT skip connections\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING 15-LAYER CNN (NO SKIP CONNECTIONS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\")\n",
    "print(\"Recording gradient L1-norms during first epoch...\\n\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum_alpha = 0.9\n",
    "num_epochs = 10\n",
    "\n",
    "gradient_norms_no_skip = []  # Store gradient norms for first epoch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_deep15_no_skip.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in model_deep15_no_skip.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_deep15_no_skip(images)\n",
    "        loss = model_deep15_no_skip.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Record gradient L1-norms during first epoch\n",
    "        if epoch == 0:\n",
    "            batch_grad_norm = 0\n",
    "            for param in model_deep15_no_skip.parameters():\n",
    "                if param.grad is not None:\n",
    "                    batch_grad_norm += torch.sum(torch.abs(param.grad)).item()\n",
    "            gradient_norms_no_skip.append(batch_grad_norm)\n",
    "        \n",
    "        # Update weights with momentum\n",
    "        model_deep15_no_skip.SGD_Optimizer(model_deep15_no_skip.parameters(), \n",
    "                                            learning_rate, \n",
    "                                            momentum=momentum_alpha)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "training_time_no_skip = time.time() - start_time\n",
    "\n",
    "# Calculate average gradient norm from first epoch\n",
    "avg_grad_norm_no_skip = sum(gradient_norms_no_skip) / len(gradient_norms_no_skip)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Training time: {training_time_no_skip/60:.1f} minutes\")\n",
    "print(f\"Average gradient L1-norm (epoch 1): {avg_grad_norm_no_skip:.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d068f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "15-LAYER CNN (NO SKIP) - RESULTS\n",
      "======================================================================\n",
      "Test Accuracy: 10.00%\n",
      "Training Accuracy: 9.76%\n",
      "Generalization Gap: -0.24%\n",
      "Average Gradient Norm (epoch 1): 18.19\n",
      "\n",
      "üìä Comparison with 5-layer CNN:\n",
      "  5-layer with momentum:  91.05%\n",
      "  15-layer no skip:       10.00%\n",
      "  Difference:             -81.05%\n",
      "\n",
      "‚ö†Ô∏è  DEGRADATION OBSERVED: Deeper network performs worse!\n",
      "    This demonstrates the degradation problem in plain deep networks.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate 15-layer CNN WITHOUT skip connections\n",
    "model_deep15_no_skip.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_deep15_no_skip(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc_no_skip = 100 * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"15-LAYER CNN (NO SKIP) - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_acc_no_skip:.2f}%\")\n",
    "print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Generalization Gap: {train_acc - test_acc_no_skip:.2f}%\")\n",
    "print(f\"Average Gradient Norm (epoch 1): {avg_grad_norm_no_skip:.2f}\")\n",
    "print(\"\\nüìä Comparison with 5-layer CNN:\")\n",
    "print(f\"  5-layer with momentum:  {best_accuracy:.2f}%\")\n",
    "print(f\"  15-layer no skip:       {test_acc_no_skip:.2f}%\")\n",
    "print(f\"  Difference:             {test_acc_no_skip - best_accuracy:+.2f}%\")\n",
    "\n",
    "if test_acc_no_skip < best_accuracy:\n",
    "    print(\"\\n‚ö†Ô∏è  DEGRADATION OBSERVED: Deeper network performs worse!\")\n",
    "    print(\"    This demonstrates the degradation problem in plain deep networks.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Deeper network improved performance!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb4a5e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "15-LAYER CNN - SKIP CONFIGURATION 1 (SHORT SKIPS)\n",
      "======================================================================\n",
      "Total parameters: 384,858\n",
      "\n",
      "Skip Connection Configuration 1:\n",
      "  Skip 1: conv1 output ‚Üí conv2 output (length 1, y = x ‚äï f(x))\n",
      "  Skip 2: conv5 output ‚Üí conv6 output (length 1, y = x ‚äï f(x))\n",
      "  Skip 3: conv9 output ‚Üí conv10 output (length 1, y = x ‚äï f(x))\n",
      "\n",
      "Total: 3 skip connections (all length 1)\n",
      "Strategy: Frequent, short gradientpathways (ResNet-style)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 15-Layer Deep CNN WITH Skip Connections - CONFIGURATION 1 (Short Skips, Length 1)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_SkipConfig1(nn.Module):\n",
    "    \"\"\"\n",
    "    Configuration 1: Short skip connections (length 1)\n",
    "    - Skip 1: conv1 ‚Üí conv2 (skips 1 layer)\n",
    "    - Skip 2: conv5 ‚Üí conv6 (skips 1 layer)\n",
    "    - Skip 3: conv9 ‚Üí conv10 (skips 1 layer)\n",
    "    Total: 3 skip connections (ResNet-style)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_SkipConfig1, self).__init__()\n",
    "        \n",
    "        # Block 1: 1‚Üí16‚Üí16‚Üí16‚Üí16 (grayscale input)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 576‚Üí256‚Üí256‚Üí10 (28√ó28‚Üí14√ó14‚Üí7√ó7‚Üí3√ó3)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1 with SHORT skip (length 1): conv1 ‚Üí conv2\n",
    "        identity1 = self.leaky_relu(self.conv1(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv2(identity1))\n",
    "        x = x + identity1  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2 with SHORT skip (length 1): conv5 ‚Üí conv6\n",
    "        identity2 = self.leaky_relu(self.conv5(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv6(identity2))\n",
    "        x = x + identity2  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3 with SHORT skip (length 1): conv9 ‚Üí conv10\n",
    "        identity3 = self.leaky_relu(self.conv9(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv10(identity3))\n",
    "        x = x + identity3  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers (no skip connections in this configuration)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test Config 1 model\n",
    "model_skip_config1 = DeepCNN15_SkipConfig1().to(device)\n",
    "total_params_c1 = sum(p.numel() for p in model_skip_config1.parameters())\n",
    "print(\"=\"*70)\n",
    "print(\"15-LAYER CNN - SKIP CONFIGURATION 1 (SHORT SKIPS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params_c1:,}\")\n",
    "print(\"\\nSkip Connection Configuration 1:\")\n",
    "print(\"  Skip 1: conv1 output ‚Üí conv2 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"  Skip 2: conv5 output ‚Üí conv6 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"  Skip 3: conv9 output ‚Üí conv10 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"\\nTotal: 3 skip connections (all length 1)\")\n",
    "print(\"Strategy: Frequent, short gradientpathways (ResNet-style)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a94233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "15-LAYER CNN - SKIP CONFIGURATION 2 (LONGER SKIPS)\n",
      "======================================================================\n",
      "Total parameters: 384,858\n",
      "\n",
      "Skip Connection Configuration 2:\n",
      "  Skip 1: conv1 output ‚Üí conv3 output (length 2, y = x ‚äï g(f(x)))\n",
      "  Skip 2: conv5 output ‚Üí conv8 output (length 3, y = x ‚äï h(g(f(x))))\n",
      "  Skip 3: conv9 output ‚Üí conv12 output (length 3, y = x ‚äï h(g(f(x))))\n",
      "\n",
      "Total: 3 skip connections (1√ó length 2, 2√ó length 3)\n",
      "Strategy: Sparser, longer gradient pathways\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 15-Layer Deep CNN WITH Skip Connections - CONFIGURATION 2 (Longer Skips, Length 2-3)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_SkipConfig2(nn.Module):\n",
    "    \"\"\"\n",
    "    Configuration 2: Longer skip connections (length 2-3)\n",
    "    - Skip 1: conv1 ‚Üí conv3 (skips 2 layers)\n",
    "    - Skip 2: conv5 ‚Üí conv8 (skips 3 layers)\n",
    "    - Skip 3: conv9 ‚Üí conv12 (skips 3 layers)\n",
    "    Total: 3 skip connections\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_SkipConfig2, self).__init__()\n",
    "        \n",
    "        # Block 1: 1‚Üí16‚Üí16‚Üí16‚Üí16 (grayscale input)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 576‚Üí256‚Üí256‚Üí10 (28√ó28‚Üí14√ó14‚Üí7√ó7‚Üí3√ó3)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1 with LONGER skip (length 2): conv1 ‚Üí conv3\n",
    "        identity1 = self.leaky_relu(self.conv1(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv2(identity1))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = x + identity1  # Skip connection: y = x ‚äï g(f(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2 with LONGER skip (length 3): conv5 ‚Üí conv8\n",
    "        identity2 = self.leaky_relu(self.conv5(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv6(identity2))\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = x + identity2  # Skip connection: y = x ‚äï h(g(f(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3 with LONGER skip (length 3): conv9 ‚Üí conv12\n",
    "        identity3 = self.leaky_relu(self.conv9(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv10(identity3))\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = x + identity3  # Skip connection: y = x ‚äï h(g(f(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers (no skip connections in this configuration)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test Config 2 model\n",
    "model_skip_config2 = DeepCNN15_SkipConfig2().to(device)\n",
    "total_params_c2 = sum(p.numel() for p in model_skip_config2.parameters())\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"15-LAYER CNN - SKIP CONFIGURATION 2 (LONGER SKIPS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params_c2:,}\")\n",
    "print(\"\\nSkip Connection Configuration 2:\")\n",
    "print(\"  Skip 1: conv1 output ‚Üí conv3 output (length 2, y = x ‚äï g(f(x)))\")\n",
    "print(\"  Skip 2: conv5 output ‚Üí conv8 output (length 3, y = x ‚äï h(g(f(x))))\")\n",
    "print(\"  Skip 3: conv9 output ‚Üí conv12 output (length 3, y = x ‚äï h(g(f(x))))\")\n",
    "print(\"\\nTotal: 3 skip connections (1√ó length 2, 2√ó length 3)\")\n",
    "print(\"Strategy: Sparser, longer gradient pathways\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc7c6d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: TRAINING THREE 15-LAYER MODELS\n",
      "================================================================================\n",
      "Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîπ TRAINING: NO SKIP CONNECTIONS (Baseline)\n",
      "======================================================================\n",
      "Epoch [1/10], Loss: 2.3030, Train Acc: 10.15%, Time: 0.6min\n",
      "Epoch [2/10], Loss: 2.3031, Train Acc: 9.89%, Time: 1.1min\n",
      "Epoch [3/10], Loss: 2.3031, Train Acc: 10.07%, Time: 1.5min\n",
      "Epoch [4/10], Loss: 2.3032, Train Acc: 9.70%, Time: 2.0min\n",
      "Epoch [5/10], Loss: 2.3031, Train Acc: 9.85%, Time: 2.5min\n",
      "Epoch [6/10], Loss: 2.3030, Train Acc: 9.96%, Time: 3.0min\n",
      "Epoch [7/10], Loss: 2.3030, Train Acc: 9.97%, Time: 3.5min\n",
      "Epoch [8/10], Loss: 1.2517, Train Acc: 52.12%, Time: 3.9min\n",
      "Epoch [9/10], Loss: 0.4835, Train Acc: 81.81%, Time: 4.4min\n",
      "Epoch [10/10], Loss: 0.3813, Train Acc: 85.64%, Time: 4.9min\n",
      "\n",
      "‚úÖ Training complete!\n",
      "Test Accuracy: 86.02%\n",
      "Average Gradient L1-norm (epoch 1): 10.62\n",
      "Training time: 4.9 minutes\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üîπ TRAINING: SKIP CONFIG 1 (Short Skips)\n",
      "======================================================================\n",
      "Epoch [1/10], Loss: 2.3033, Train Acc: 9.80%, Time: 0.6min\n",
      "Epoch [2/10], Loss: 2.3033, Train Acc: 9.92%, Time: 1.1min\n",
      "Epoch [3/10], Loss: 1.1144, Train Acc: 57.49%, Time: 1.6min\n",
      "Epoch [4/10], Loss: 0.4109, Train Acc: 84.58%, Time: 2.1min\n",
      "Epoch [5/10], Loss: 0.3431, Train Acc: 87.01%, Time: 2.5min\n",
      "Epoch [6/10], Loss: 0.3008, Train Acc: 88.75%, Time: 3.0min\n",
      "Epoch [7/10], Loss: 0.2734, Train Acc: 89.90%, Time: 3.5min\n",
      "Epoch [8/10], Loss: 0.2537, Train Acc: 90.54%, Time: 4.0min\n",
      "Epoch [9/10], Loss: 0.2379, Train Acc: 90.99%, Time: 4.5min\n",
      "Epoch [10/10], Loss: 0.2256, Train Acc: 91.49%, Time: 5.0min\n",
      "\n",
      "‚úÖ Training complete!\n",
      "Test Accuracy: 89.68%\n",
      "Average Gradient L1-norm (epoch 1): 18.59\n",
      "Training time: 5.0 minutes\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üîπ TRAINING: SKIP CONFIG 2 (Longer Skips)\n",
      "======================================================================\n",
      "Epoch [1/10], Loss: 0.6767, Train Acc: 75.07%, Time: 0.6min\n",
      "Epoch [2/10], Loss: 0.3606, Train Acc: 86.47%, Time: 1.1min\n",
      "Epoch [3/10], Loss: 0.3085, Train Acc: 88.49%, Time: 1.5min\n",
      "Epoch [4/10], Loss: 0.2796, Train Acc: 89.42%, Time: 2.0min\n",
      "Epoch [5/10], Loss: 0.2568, Train Acc: 90.22%, Time: 2.5min\n",
      "Epoch [6/10], Loss: 0.2339, Train Acc: 91.19%, Time: 3.0min\n",
      "Epoch [7/10], Loss: 0.2190, Train Acc: 91.75%, Time: 3.4min\n",
      "Epoch [8/10], Loss: 0.2035, Train Acc: 92.28%, Time: 3.9min\n",
      "Epoch [9/10], Loss: 0.1886, Train Acc: 92.82%, Time: 4.4min\n",
      "Epoch [10/10], Loss: 0.1748, Train Acc: 93.49%, Time: 4.9min\n",
      "\n",
      "‚úÖ Training complete!\n",
      "Test Accuracy: 90.35%\n",
      "Average Gradient L1-norm (epoch 1): 696.42\n",
      "Training time: 4.9 minutes\n",
      "======================================================================\n",
      "\n",
      "================================================================================\n",
      "üéØ ALL THREE MODELS TRAINED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train all THREE models: No skip, Config 1, Config 2\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: TRAINING THREE 15-LAYER MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\\n\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum_alpha = 0.9\n",
    "num_epochs = 10\n",
    "\n",
    "# Dictionary to store results for all 3 models\n",
    "deep_results = {\n",
    "    'no_skip': {},\n",
    "    'config1': {},\n",
    "    'config2': {}\n",
    "}\n",
    "\n",
    "# Model configurations\n",
    "models_to_train = [\n",
    "    ('no_skip', model_deep15_no_skip, \"NO SKIP CONNECTIONS (Baseline)\"),\n",
    "    ('config1', model_skip_config1, \"SKIP CONFIG 1 (Short Skips)\"),\n",
    "    ('config2', model_skip_config2, \"SKIP CONFIG 2 (Longer Skips)\")\n",
    "]\n",
    "\n",
    "for model_key, model, model_name in models_to_train:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîπ TRAINING: {model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gradient_norms = []  # Store gradient norms for first epoch\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = model.cross_entropy_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Record gradient L1-norms during first epoch\n",
    "            if epoch == 0:\n",
    "                batch_grad_norm = 0\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        batch_grad_norm += torch.sum(torch.abs(param.grad)).item()\n",
    "                gradient_norms.append(batch_grad_norm)\n",
    "            \n",
    "            # Update weights with momentum\n",
    "            model.SGD_Optimizer(model.parameters(), learning_rate, momentum=momentum_alpha)\n",
    "        \n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    avg_grad_norm = sum(gradient_norms) / len(gradient_norms)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    \n",
    "    # Store results\n",
    "    deep_results[model_key] = {\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': avg_loss,\n",
    "        'training_time': training_time/60,\n",
    "        'avg_grad_norm': avg_grad_norm\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Average Gradient L1-norm (epoch 1): {avg_grad_norm:.2f}\")\n",
    "    print(f\"Training time: {training_time/60:.1f} minutes\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ ALL THREE MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2650d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: COMPREHENSIVE RESULTS - SKIP CONNECTIONS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä MODEL COMPARISON TABLE:\n",
      "Model                                         Test Acc     Train Acc    Grad Norm       Time      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "5-layer CNN (Part 2 baseline)                 91.05        -            -               2.8 min   \n",
      "15-layer WITHOUT skip connections             86.02        85.64        10.62           4.9 min\n",
      "15-layer WITH skip - Config 1 (short)         89.68        91.49        18.59           5.0 min\n",
      "15-layer WITH skip - Config 2 (longer)        90.35        93.49        696.42          4.9 min\n",
      "===============================================================================================\n",
      "\n",
      "üìà GRADIENT FLOW ANALYSIS:\n",
      "  No skip connections:       10.62\n",
      "  Config 1 (short skips):    18.59  (ratio: 1.75x)\n",
      "  Config 2 (longer skips):   696.42  (ratio: 65.55x)\n",
      "\n",
      "  ‚úÖ Config 1 increased gradient magnitudes by 75.0%\n",
      "  ‚úÖ Config 2 increased gradient magnitudes by 6454.9%\n",
      "  ‚Üí Skip connections combat vanishing gradients!\n",
      "\n",
      "üìä ACCURACY ANALYSIS:\n",
      "  Extended model (no skip) vs 5-layer:  -5.03%\n",
      "  Config 1 vs No Skip:                   +3.66% improvement\n",
      "  Config 2 vs No Skip:                   +4.33% improvement\n",
      "  Config 1 vs 5-layer baseline:          -1.37%\n",
      "  Config 2 vs 5-layer baseline:          -0.70%\n",
      "\n",
      "üèÜ BEST SKIP CONFIGURATION: Config 2 (longer skips) with 90.35%\n",
      "\n",
      "üîç KEY FINDINGS:\n",
      "  1. ‚ö†Ô∏è  DEGRADATION PROBLEM OBSERVED:\n",
      "      ‚Ä¢ 15-layer without skip: 86.02%\n",
      "      ‚Ä¢ 5-layer baseline:      91.05%\n",
      "      ‚Ä¢ Difference:            -5.03%\n",
      "      ‚Üí Adding layers WITHOUT skip connections HURTS performance!\n",
      "      ‚Üí This is NOT overfitting - it's an optimization failure\n",
      "  2. ‚úÖ SKIP CONNECTIONS SOLVE DEGRADATION:\n",
      "      ‚Ä¢ Config 1 (short):  89.68% (+3.66% vs no skip)\n",
      "      ‚Ä¢ Config 2 (longer): 90.35% (+4.33% vs no skip)\n",
      "      ‚Üí Skip connections enable training of deep networks!\n",
      "  3. üìä GRADIENT FLOW IMPROVEMENT:\n",
      "      ‚Ä¢ Config 1: +75.0% stronger gradients\n",
      "      ‚Ä¢ Config 2: +6454.9% stronger gradients\n",
      "      ‚Üí Skip connections provide gradient highways to early layers\n",
      "  4. üî¨ SKIP CONFIGURATION COMPARISON:\n",
      "      ‚Ä¢ Config 2 (longer/sparser) outperformed Config 1 by 0.67%\n",
      "      ‚Üí Longer skips provide stronger gradient flow\n",
      "      ‚Üí Deeper shortcuts may be more effective\n",
      "  5. ‚úì COMPETITIVE PERFORMANCE:\n",
      "      ‚Ä¢ Best 15-layer (with skip): 90.35%\n",
      "      ‚Ä¢ 5-layer baseline:          91.05%\n",
      "      ‚Üí Skip connections enabled training a 3√ó deeper network\n",
      "\n",
      "üí° CONCLUSIONS:\n",
      "  ‚Ä¢ Skip connections are ESSENTIAL for training deep neural networks\n",
      "  ‚Ä¢ They solve the degradation problem by providing gradient highways\n",
      "  ‚Ä¢ Stronger gradient flow ‚Üí Better optimization ‚Üí Higher accuracy\n",
      "  ‚Ä¢ Best approach: Config 2 (longer skips)\n",
      "  ‚Ä¢ Best 15-layer model: 90.35% test accuracy\n",
      "  ‚Ä¢ Without skip connections, adding layers can hurt performance!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# PART 3: Comprehensive Comparison and Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: COMPREHENSIVE RESULTS - SKIP CONNECTIONS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract results for easier access\n",
    "no_skip = deep_results['no_skip']\n",
    "config1 = deep_results['config1']\n",
    "config2 = deep_results['config2']\n",
    "\n",
    "print(\"\\nüìä MODEL COMPARISON TABLE:\")\n",
    "print(f\"{'Model':<45} {'Test Acc':<12} {'Train Acc':<12} {'Grad Norm':<15} {'Time':<10}\")\n",
    "print(\"-\"*95)\n",
    "print(f\"{'5-layer CNN (Part 2 baseline)':<45} {best_accuracy:<12.2f} {'-':<12} {'-':<15} {'2.8 min':<10}\")\n",
    "print(f\"{'15-layer WITHOUT skip connections':<45} {no_skip['test_accuracy']:<12.2f} {no_skip['train_accuracy']:<12.2f} {no_skip['avg_grad_norm']:<15.2f} {no_skip['training_time']:.1f} min\")\n",
    "print(f\"{'15-layer WITH skip - Config 1 (short)':<45} {config1['test_accuracy']:<12.2f} {config1['train_accuracy']:<12.2f} {config1['avg_grad_norm']:<15.2f} {config1['training_time']:.1f} min\")\n",
    "print(f\"{'15-layer WITH skip - Config 2 (longer)':<45} {config2['test_accuracy']:<12.2f} {config2['train_accuracy']:<12.2f} {config2['avg_grad_norm']:<15.2f} {config2['training_time']:.1f} min\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "print(\"\\nüìà GRADIENT FLOW ANALYSIS:\")\n",
    "print(f\"  No skip connections:       {no_skip['avg_grad_norm']:.2f}\")\n",
    "print(f\"  Config 1 (short skips):    {config1['avg_grad_norm']:.2f}  (ratio: {config1['avg_grad_norm']/no_skip['avg_grad_norm']:.2f}x)\")\n",
    "print(f\"  Config 2 (longer skips):   {config2['avg_grad_norm']:.2f}  (ratio: {config2['avg_grad_norm']/no_skip['avg_grad_norm']:.2f}x)\")\n",
    "\n",
    "if config1['avg_grad_norm'] > no_skip['avg_grad_norm']:\n",
    "    improvement = ((config1['avg_grad_norm']/no_skip['avg_grad_norm']) - 1) * 100\n",
    "    print(f\"\\n  ‚úÖ Config 1 increased gradient magnitudes by {improvement:.1f}%\")\n",
    "if config2['avg_grad_norm'] > no_skip['avg_grad_norm']:\n",
    "    improvement = ((config2['avg_grad_norm']/no_skip['avg_grad_norm']) - 1) * 100\n",
    "    print(f\"  ‚úÖ Config 2 increased gradient magnitudes by {improvement:.1f}%\")\n",
    "print(f\"  ‚Üí Skip connections combat vanishing gradients!\")\n",
    "\n",
    "print(\"\\nüìä ACCURACY ANALYSIS:\")\n",
    "print(f\"  Extended model (no skip) vs 5-layer:  {no_skip['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "print(f\"  Config 1 vs No Skip:                   {config1['test_accuracy'] - no_skip['test_accuracy']:+.2f}% improvement\")\n",
    "print(f\"  Config 2 vs No Skip:                   {config2['test_accuracy'] - no_skip['test_accuracy']:+.2f}% improvement\")\n",
    "print(f\"  Config 1 vs 5-layer baseline:          {config1['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "print(f\"  Config 2 vs 5-layer baseline:          {config2['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "\n",
    "# Determine best skip configuration\n",
    "best_skip_model = 'config1' if config1['test_accuracy'] >= config2['test_accuracy'] else 'config2'\n",
    "best_skip_name = \"Config 1 (short skips)\" if best_skip_model == 'config1' else \"Config 2 (longer skips)\"\n",
    "best_skip_acc = deep_results[best_skip_model]['test_accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ BEST SKIP CONFIGURATION: {best_skip_name} with {best_skip_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nüîç KEY FINDINGS:\")\n",
    "\n",
    "# Finding 1: Degradation problem\n",
    "if no_skip['test_accuracy'] < best_accuracy:\n",
    "    print(f\"  1. ‚ö†Ô∏è  DEGRADATION PROBLEM OBSERVED:\")\n",
    "    print(f\"      ‚Ä¢ 15-layer without skip: {no_skip['test_accuracy']:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:      {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ Difference:            {no_skip['test_accuracy'] - best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Üí Adding layers WITHOUT skip connections HURTS performance!\")\n",
    "    print(f\"      ‚Üí This is NOT overfitting - it's an optimization failure\")\n",
    "else:\n",
    "    print(f\"  1. Extended model (no skip) achieved {no_skip['test_accuracy']:.2f}%\")\n",
    "    if no_skip['test_accuracy'] > best_accuracy:\n",
    "        print(f\"      ‚Üí Deeper network improved over 5-layer!\")\n",
    "    else:\n",
    "        print(f\"      ‚Üí Similar performance to 5-layer baseline\")\n",
    "\n",
    "# Finding 2: Skip connections solve degradation\n",
    "if config1['test_accuracy'] > no_skip['test_accuracy'] or config2['test_accuracy'] > no_skip['test_accuracy']:\n",
    "    print(f\"  2. ‚úÖ SKIP CONNECTIONS SOLVE DEGRADATION:\")\n",
    "    print(f\"      ‚Ä¢ Config 1 (short):  {config1['test_accuracy']:.2f}% ({config1['test_accuracy'] - no_skip['test_accuracy']:+.2f}% vs no skip)\")\n",
    "    print(f\"      ‚Ä¢ Config 2 (longer): {config2['test_accuracy']:.2f}% ({config2['test_accuracy'] - no_skip['test_accuracy']:+.2f}% vs no skip)\")\n",
    "    print(f\"      ‚Üí Skip connections enable training of deep networks!\")\n",
    "\n",
    "# Finding 3: Gradient flow improvement\n",
    "grad_improve_c1 = (config1['avg_grad_norm'] / no_skip['avg_grad_norm'] - 1) * 100\n",
    "grad_improve_c2 = (config2['avg_grad_norm'] / no_skip['avg_grad_norm'] - 1) * 100\n",
    "print(f\"  3. üìä GRADIENT FLOW IMPROVEMENT:\")\n",
    "print(f\"      ‚Ä¢ Config 1: {grad_improve_c1:+.1f}% stronger gradients\")\n",
    "print(f\"      ‚Ä¢ Config 2: {grad_improve_c2:+.1f}% stronger gradients\")\n",
    "print(f\"      ‚Üí Skip connections provide gradient highways to early layers\")\n",
    "\n",
    "# Finding 4: Configuration comparison\n",
    "print(f\"  4. üî¨ SKIP CONFIGURATION COMPARISON:\")\n",
    "if config1['test_accuracy'] > config2['test_accuracy']:\n",
    "    diff = config1['test_accuracy'] - config2['test_accuracy']\n",
    "    print(f\"      ‚Ä¢ Config 1 (short/frequent) outperformed Config 2 by {diff:.2f}%\")\n",
    "    print(f\"      ‚Üí Frequent short skips (ResNet-style) work better for this architecture\")\n",
    "    print(f\"      ‚Üí More gradient pathways = better optimization\")\n",
    "elif config2['test_accuracy'] > config1['test_accuracy']:\n",
    "    diff = config2['test_accuracy'] - config1['test_accuracy']\n",
    "    print(f\"      ‚Ä¢ Config 2 (longer/sparser) outperformed Config 1 by {diff:.2f}%\")\n",
    "    print(f\"      ‚Üí Longer skips provide stronger gradient flow\")\n",
    "    print(f\"      ‚Üí Deeper shortcuts may be more effective\")\n",
    "else:\n",
    "    print(f\"      ‚Ä¢ Both configurations achieved similar performance ({config1['test_accuracy']:.2f}%)\")\n",
    "    print(f\"      ‚Üí Both skip strategies are effective for gradient flow\")\n",
    "\n",
    "# Check if skip connections beat 5-layer baseline\n",
    "if best_skip_acc > best_accuracy:\n",
    "    improvement = best_skip_acc - best_accuracy\n",
    "    print(f\"  5. ‚úÖ DEEP NETWORK SUCCESS:\")\n",
    "    print(f\"      ‚Ä¢ Best 15-layer (with skip): {best_skip_acc:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:          {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ Improvement:               +{improvement:.2f}%\")\n",
    "    print(f\"      ‚Üí Skip connections enabled a deeper network to outperform shallow!\")\n",
    "elif best_skip_acc >= best_accuracy - 2:\n",
    "    print(f\"  5. ‚úì COMPETITIVE PERFORMANCE:\")\n",
    "    print(f\"      ‚Ä¢ Best 15-layer (with skip): {best_skip_acc:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:          {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Üí Skip connections enabled training a 3√ó deeper network\")\n",
    "else:\n",
    "    print(f\"  5. ‚ö†Ô∏è  Still below 5-layer baseline, but skip connections helped significantly\")\n",
    "\n",
    "print(\"\\nüí° CONCLUSIONS:\")\n",
    "print(\"  ‚Ä¢ Skip connections are ESSENTIAL for training deep neural networks\")\n",
    "print(\"  ‚Ä¢ They solve the degradation problem by providing gradient highways\")\n",
    "print(\"  ‚Ä¢ Stronger gradient flow ‚Üí Better optimization ‚Üí Higher accuracy\")\n",
    "print(f\"  ‚Ä¢ Best approach: {best_skip_name}\")\n",
    "print(f\"  ‚Ä¢ Best 15-layer model: {best_skip_acc:.2f}% test accuracy\")\n",
    "print(\"  ‚Ä¢ Without skip connections, adding layers can hurt performance!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
