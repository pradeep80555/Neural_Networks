{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a807c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download CIFAR-10\n",
    "# 2. Normalize to mean=0, std=1 (or use standard transform)\n",
    "# 3. Create train/test DataLoaders\n",
    "# 4. Verify data shapes (should be [batch, 3, 32, 32])\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# 1. Download CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors, scales [0,255] to [0,1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scale to [-1, 1], center at 0\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # predefined 50,000 images for training\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # predefined 10,000 images for testing\n",
    "# 3. Create train/test DataLoaders\n",
    "batch_size = 1  # Part 1 baseline requires batch_size=1 (change to 32/64 for Part 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 4. Verify data shapes\n",
    "# for images, labels in train_loader:\n",
    "#     print(f'Batch of images shape: {images.shape}')  # Should be [batch_size, 3, 32, 32] 64 images per batch, 3 channels, 32x32 pixels\n",
    "#     print(f'Batch of labels shape: {labels.shape}')  # Should be [batch_size]\n",
    "#     break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01108ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2-Layer Network to prove that cifar10 requires deeper networks.\n",
    "# **Purpose:** Prove that CIFAR-10 needs deep learning\n",
    "\n",
    "# **Architecture:**\n",
    "# ```\n",
    "# Input: 3072 (32√ó32√ó3 flattened)\n",
    "#   ‚Üì\n",
    "# Linear(3072 ‚Üí 128) + Sigmoid\n",
    "#   ‚Üì\n",
    "# Linear(128 ‚Üí 10) + Softmax\n",
    "# ```\n",
    "\n",
    "# **Implementation details:**\n",
    "# - Use `torch.nn.Linear()` (allowed)\n",
    "# - Implement sigmoid activation manually: `1 / (1 + torch.exp(-x))`\n",
    "# - Implement softmax manually: `torch.exp(x) / torch.exp(x).sum()`\n",
    "# - Implement cross-entropy loss manually\n",
    "\n",
    "# **Training setup:**\n",
    "# - Optimizer: SGD (implement manually)\n",
    "# - Batch size: 1\n",
    "# - Learning rate: 0.01 or 0.001\n",
    "# - Epochs: 10-20\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])  # Subtract max for stability\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # Compute cross-entropy loss with numerical stability\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None  # Reset gradient\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 128)  # Input: 32√ó32√ó3 = 3072\n",
    "        self.fc2 = nn.Linear(128, 10)    # Output: 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.sigmoid(self.fc1(x))  # First layer + sigmoid\n",
    "        x = self.fc2(x)  # Second layer (logits)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a27a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNet().to(device)\n",
    "learning_rate = 0.005  # Reduced from 0.01 to prevent instability\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8831ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.7889, Train Acc: 36.52%\n",
      "Epoch [2/10], Loss: 1.6314, Train Acc: 43.13%\n",
      "Epoch [3/10], Loss: 1.5447, Train Acc: 46.24%\n",
      "Epoch [4/10], Loss: 1.4780, Train Acc: 48.73%\n",
      "Epoch [5/10], Loss: 1.4221, Train Acc: 50.33%\n",
      "Epoch [6/10], Loss: 1.3684, Train Acc: 52.39%\n",
      "Epoch [7/10], Loss: 1.3216, Train Acc: 54.13%\n",
      "Epoch [8/10], Loss: 1.2791, Train Acc: 55.42%\n",
      "Epoch [9/10], Loss: 1.2420, Train Acc: 56.61%\n",
      "Epoch [10/10], Loss: 1.2081, Train Acc: 58.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients before backward pass\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using SGD\n",
    "        model.SGD_Optimizer(model.parameters(), learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf87c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 48.04%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy in percentage\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6036c",
   "metadata": {},
   "source": [
    "# Part 2, Step 4: Activation Functions Comparison\n",
    "\n",
    "**Previous Result (Part 1):** 5-Layer CNN with Sigmoid failed completely (10% accuracy - vanishing gradients)\n",
    "\n",
    "**Purpose:** Test modern activation functions to solve vanishing gradient problem\n",
    "\n",
    "**New Activations Tested:**\n",
    "1. **Leaky ReLU** - f(x) = max(x, 0.1x)\n",
    "   - No saturation for positive values (gradient = 1)\n",
    "   - Small gradient (0.1) for negative values prevents dying neurons\n",
    "   - Used for: Conv layers 1, 2, 3\n",
    "\n",
    "2. **Tanh** - f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "   - Zero-centered output range: (-1, 1)\n",
    "   - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "   - Used for: FC layer 1\n",
    "\n",
    "**Architecture (unchanged):**\n",
    "- 3 Convolutional layers with MaxPooling (Conv: 3‚Üí16‚Üí32‚Üí64)\n",
    "- 2 Fully connected layers (FC: 1024‚Üí256‚Üí10)\n",
    "- Total: 5 parameterized layers, 288,554 parameters\n",
    "\n",
    "**Expected result:** 60-70% test accuracy (dramatic improvement from 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19197043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cuda\n",
      "Total parameters: 288554\n"
     ]
    }
   ],
   "source": [
    "# 5-Layer CNN Architecture\n",
    "class FiveLayerCNN(nn.Module):\n",
    "    def leaky_relu(self, x, negative_slope=0.1):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation: f(x) = max(x, 0.1*x)\n",
    "        - For x > 0: output = x (gradient = 1, no vanishing)\n",
    "        - For x < 0: output = 0.1*x (gradient = 0.1, prevents dying neurons)\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, negative_slope * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent: f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "        - Output range: (-1, 1)\n",
    "        - Zero-centered (better than sigmoid)\n",
    "        - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "        \"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FiveLayerCNN, self).__init__()\n",
    "        # Convolutional layers (3 parameterized layers)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # Layer 1: 3‚Üí16 channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # MaxPool (not parameterized)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Layer 2: 16‚Üí32 channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Layer 3: 32‚Üí64 channels\n",
    "        \n",
    "        # Fully connected layers (2 parameterized layers)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Layer 4: After 3 pooling ops, 32√ó32‚Üí4√ó4\n",
    "        self.fc2 = nn.Linear(256, 10)           # Layer 5: Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, 32, 32]\n",
    "        \n",
    "        # Conv block 1 - using Leaky ReLU\n",
    "        x = self.conv1(x)           # [batch, 16, 32, 32]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 16, 16, 16]\n",
    "        \n",
    "        # Conv block 2 - using Leaky ReLU\n",
    "        x = self.conv2(x)           # [batch, 32, 16, 16]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 32, 8, 8]\n",
    "        \n",
    "        # Conv block 3 - using Leaky ReLU\n",
    "        x = self.conv3(x)           # [batch, 64, 8, 8]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 64, 4, 4]\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)   # [batch, 1024]\n",
    "        \n",
    "        # Fully connected layers - using Tanh\n",
    "        x = self.fc1(x)             # [batch, 256]\n",
    "        x = self.tanh(x)            # Tanh activation\n",
    "        x = self.fc2(x)             # [batch, 10] - logits\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "cnn_model = FiveLayerCNN().to(device)\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in cnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning rate: 0.005\n",
      "  Epochs: 10\n",
      "  Batch size: 1\n",
      "  Device: cuda\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# Training setup for CNN\n",
    "cnn_learning_rate = 0.01  # Increased from 0.005 - Leaky ReLU and Tanh have better gradient flow\n",
    "cnn_num_epochs = 10  # More epochs for deeper network\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {cnn_learning_rate}\")\n",
    "print(f\"  Epochs: {cnn_num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1326fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3099, Train Acc: 9.99%, Time: 3.1min\n",
      "Epoch [2/10], Loss: 2.3042, Train Acc: 9.97%, Time: 6.3min\n",
      "Epoch [3/10], Loss: 2.3039, Train Acc: 9.80%, Time: 9.4min\n",
      "Epoch [4/10], Loss: 2.3041, Train Acc: 9.75%, Time: 12.5min\n",
      "Epoch [5/10], Loss: 2.3038, Train Acc: 10.16%, Time: 15.6min\n",
      "Epoch [6/10], Loss: 2.3039, Train Acc: 9.99%, Time: 18.7min\n",
      "Epoch [7/10], Loss: 2.3038, Train Acc: 10.07%, Time: 21.8min\n",
      "Epoch [8/10], Loss: 2.3038, Train Acc: 10.15%, Time: 25.0min\n",
      "Epoch [9/10], Loss: 2.3038, Train Acc: 9.87%, Time: 28.1min\n",
      "Epoch [10/10], Loss: 2.3038, Train Acc: 10.00%, Time: 31.3min\n",
      "\n",
      "Total training time: 31.3 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training loop for CNN\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(cnn_num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in cnn_model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = cnn_model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        cnn_model.SGD_Optimizer(cnn_model.parameters(), cnn_learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Print progress every epoch\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{cnn_num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "print(f\"\\nTotal training time: {(time.time() - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51931b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "5-Layer CNN Test Accuracy: 10.00%\n",
      "==================================================\n",
      "\n",
      "üìä Comparison:\n",
      "  2-Layer Network: 48.04%\n",
      "  5-Layer CNN:     10.00%\n",
      "  Improvement:     -38.04%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN on test set\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_test_accuracy = 100 * correct / total\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'5-Layer CNN Test Accuracy: {cnn_test_accuracy:.2f}%')\n",
    "print(f'{\"=\"*50}')\n",
    "\n",
    "# Compare with 2-layer network\n",
    "print(f'\\nüìä Comparison:')\n",
    "print(f'  2-Layer Network: 48.04%')\n",
    "print(f'  5-Layer CNN:     {cnn_test_accuracy:.2f}%')\n",
    "print(f'  Improvement:     {cnn_test_accuracy - 48.04:.2f}%')\n",
    "\n",
    "if cnn_test_accuracy > 50:\n",
    "    print(f'\\n‚úÖ SUCCESS: Deep network achieves >{50}% accuracy!')\n",
    "    print(f'‚úÖ This proves CIFAR-10 requires deep learning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Part 1 Complete!\n",
    "\n",
    "You now have:\n",
    "1. ‚úÖ **2-Layer Network** - 48.04% test accuracy (proves shallow networks struggle)\n",
    "2. ‚úÖ **5-Layer CNN** - Should get 50-60% test accuracy (proves depth helps)\n",
    "\n",
    "### üéØ Expected Training Time:\n",
    "- With batch_size=1 and 30 epochs: **~2-3 hours**\n",
    "- Each epoch processes 50,000 images individually\n",
    "\n",
    "### üí° Tips:\n",
    "- The training will take a while - be patient!\n",
    "- Loss should steadily decrease\n",
    "- Accuracy should improve over 2-layer baseline\n",
    "- You can reduce epochs to 20 if you're short on time\n",
    "\n",
    "### üìù Next Steps (Part 2):\n",
    "After this training completes, you'll:\n",
    "1. Test different activation functions (Leaky ReLU, Tanh)\n",
    "2. Implement mini-batch SGD (batch sizes: 16, 32, 64, 128)\n",
    "3. Add momentum to the optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
