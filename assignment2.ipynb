{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a807c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:12<00:00, 13.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "# 1. Download CIFAR-10\n",
    "# 2. Normalize to mean=0, std=1 (or use standard transform)\n",
    "# 3. Create train/test DataLoaders\n",
    "# 4. Verify data shapes (should be [batch, 3, 32, 32])\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# 1. Download CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors, scales [0,255] to [0,1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scale to [-1, 1], center at 0\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # predefined 50,000 images for training\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # predefined 10,000 images for testing\n",
    "# 3. Create train/test DataLoaders\n",
    "batch_size = 32  # Part 1 baseline requires batch_size=1 (change to 32/64 for Part 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 4. Verify data shapes\n",
    "# for images, labels in train_loader:\n",
    "#     print(f'Batch of images shape: {images.shape}')  # Should be [batch_size, 3, 32, 32] 64 images per batch, 3 channels, 32x32 pixels\n",
    "#     print(f'Batch of labels shape: {labels.shape}')  # Should be [batch_size]\n",
    "#     break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01108ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2-Layer Network to prove that cifar10 requires deeper networks.\n",
    "# **Purpose:** Prove that CIFAR-10 needs deep learning\n",
    "\n",
    "# **Architecture:**\n",
    "# ```\n",
    "# Input: 3072 (32√ó32√ó3 flattened)\n",
    "#   ‚Üì\n",
    "# Linear(3072 ‚Üí 128) + Sigmoid\n",
    "#   ‚Üì\n",
    "# Linear(128 ‚Üí 10) + Softmax\n",
    "# ```\n",
    "\n",
    "# **Implementation details:**\n",
    "# - Use `torch.nn.Linear()` (allowed)\n",
    "# - Implement sigmoid activation manually: `1 / (1 + torch.exp(-x))`\n",
    "# - Implement softmax manually: `torch.exp(x) / torch.exp(x).sum()`\n",
    "# - Implement cross-entropy loss manually\n",
    "\n",
    "# **Training setup:**\n",
    "# - Optimizer: SGD (implement manually)\n",
    "# - Batch size: 1\n",
    "# - Learning rate: 0.01 or 0.001\n",
    "# - Epochs: 10-20\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])  # Subtract max for stability\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # Compute cross-entropy loss with numerical stability\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr):\n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.data = param.data - lr * param.grad\n",
    "                    param.grad = None  # Reset gradient\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072, 128)  # Input: 32√ó32√ó3 = 3072\n",
    "        self.fc2 = nn.Linear(128, 10)    # Output: 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.sigmoid(self.fc1(x))  # First layer + sigmoid\n",
    "        x = self.fc2(x)  # Second layer (logits)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a27a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNet().to(device)\n",
    "learning_rate = 0.005  # Reduced from 0.01 to prevent instability\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8831ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0981, Train Acc: 26.84%\n",
      "Epoch [2/10], Loss: 1.9342, Train Acc: 32.78%\n",
      "Epoch [3/10], Loss: 1.8707, Train Acc: 35.15%\n",
      "Epoch [4/10], Loss: 1.8327, Train Acc: 36.53%\n",
      "Epoch [5/10], Loss: 1.8042, Train Acc: 37.44%\n",
      "Epoch [6/10], Loss: 1.7817, Train Acc: 38.14%\n",
      "Epoch [7/10], Loss: 1.7635, Train Acc: 38.73%\n",
      "Epoch [8/10], Loss: 1.7489, Train Acc: 39.41%\n",
      "Epoch [9/10], Loss: 1.7358, Train Acc: 39.82%\n",
      "Epoch [10/10], Loss: 1.7243, Train Acc: 40.27%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients before backward pass\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using SGD\n",
    "        model.SGD_Optimizer(model.parameters(), learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf87c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 39.93%\n"
     ]
    }
   ],
   "source": [
    "# test accuracy in percentage\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6036c",
   "metadata": {},
   "source": [
    "# Part 2, Step 4: Activation Functions Comparison\n",
    "\n",
    "**Previous Result (Part 1):** 5-Layer CNN with Sigmoid failed completely (10% accuracy - vanishing gradients)\n",
    "\n",
    "**Purpose:** Test modern activation functions to solve vanishing gradient problem\n",
    "\n",
    "**New Activations Tested:**\n",
    "1. **Leaky ReLU** - f(x) = max(x, 0.1x)\n",
    "   - No saturation for positive values (gradient = 1)\n",
    "   - Small gradient (0.1) for negative values prevents dying neurons\n",
    "   - Used for: Conv layers 1, 2, 3\n",
    "\n",
    "2. **Tanh** - f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "   - Zero-centered output range: (-1, 1)\n",
    "   - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "   - Used for: FC layer 1\n",
    "\n",
    "**Architecture (unchanged):**\n",
    "- 3 Convolutional layers with MaxPooling (Conv: 3‚Üí16‚Üí32‚Üí64)\n",
    "- 2 Fully connected layers (FC: 1024‚Üí256‚Üí10)\n",
    "- Total: 5 parameterized layers, 288,554 parameters\n",
    "\n",
    "**Expected result:** 60-70% test accuracy (dramatic improvement from 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19197043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cuda\n",
      "Total parameters: 288554\n"
     ]
    }
   ],
   "source": [
    "# 5-Layer CNN Architecture\n",
    "class FiveLayerCNN(nn.Module):\n",
    "    def leaky_relu(self, x, negative_slope=0.1):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation: f(x) = max(x, 0.1*x)\n",
    "        - For x > 0: output = x (gradient = 1, no vanishing)\n",
    "        - For x < 0: output = 0.1*x (gradient = 0.1, prevents dying neurons)\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, negative_slope * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent: f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "        - Output range: (-1, 1)\n",
    "        - Zero-centered (better than sigmoid)\n",
    "        - Max gradient = 1 (vs sigmoid's 0.25)\n",
    "        \"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        one_hot_labels = torch.zeros_like(outputs)\n",
    "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "        log_probs = torch.log(self.softmax(outputs) + 1e-10)\n",
    "        loss = -torch.sum(one_hot_labels * log_probs) / outputs.size(0)\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"\n",
    "        SGD with momentum implementation following equation (8):\n",
    "        m_{i+1} = Œ± * m_i + g_i\n",
    "        Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "        \n",
    "        Args:\n",
    "            params: Model parameters\n",
    "            lr: Learning rate (Œ∑)\n",
    "            momentum: Momentum coefficient (Œ±), default=0.0 for vanilla SGD\n",
    "        \"\"\"\n",
    "        # Initialize momentum buffer on first call\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    # Get parameter id for momentum buffer\n",
    "                    param_id = id(param)\n",
    "                    \n",
    "                    # Initialize momentum to zero if not exists\n",
    "                    if param_id not in self.momentum_buffer:\n",
    "                        self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                    \n",
    "                    # Get current gradient (g_i)\n",
    "                    grad = param.grad\n",
    "                    \n",
    "                    # Update momentum: m_{i+1} = Œ± * m_i + g_i\n",
    "                    self.momentum_buffer[param_id] = momentum * self.momentum_buffer[param_id] + grad\n",
    "                    \n",
    "                    # Update parameters: Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                    param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "                    \n",
    "                    # Reset gradient\n",
    "                    param.grad = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FiveLayerCNN, self).__init__()\n",
    "        # Convolutional layers (3 parameterized layers)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # Layer 1: 3‚Üí16 channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # MaxPool (not parameterized)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Layer 2: 16‚Üí32 channels\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Layer 3: 32‚Üí64 channels\n",
    "        \n",
    "        # Fully connected layers (2 parameterized layers)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Layer 4: After 3 pooling ops, 32√ó32‚Üí4√ó4\n",
    "        self.fc2 = nn.Linear(256, 10)           # Layer 5: Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, 32, 32]\n",
    "        \n",
    "        # Conv block 1 - using Leaky ReLU\n",
    "        x = self.conv1(x)           # [batch, 16, 32, 32]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 16, 16, 16]\n",
    "        \n",
    "        # Conv block 2 - using Leaky ReLU\n",
    "        x = self.conv2(x)           # [batch, 32, 16, 16]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 32, 8, 8]\n",
    "        \n",
    "        # Conv block 3 - using Leaky ReLU\n",
    "        x = self.conv3(x)           # [batch, 64, 8, 8]\n",
    "        x = self.leaky_relu(x)      # Leaky ReLU activation\n",
    "        x = self.pool(x)            # [batch, 64, 4, 4]\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)   # [batch, 1024]\n",
    "        \n",
    "        # Fully connected layers - using Tanh\n",
    "        x = self.fc1(x)             # [batch, 256]\n",
    "        x = self.tanh(x)            # Tanh activation\n",
    "        x = self.fc2(x)             # [batch, 10] - logits\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "cnn_model = FiveLayerCNN().to(device)\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in cnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab78ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Learning rate: 0.005\n",
      "  Epochs: 10\n",
      "  Batch size: 32\n",
      "  Device: cuda\n",
      "  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# Training setup for CNN\n",
    "cnn_learning_rate = 0.005  # Increased from 0.005 - Leaky ReLU and Tanh have better gradient flow\n",
    "cnn_num_epochs = 10  # More epochs for deeper network\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {cnn_learning_rate}\")\n",
    "print(f\"  Epochs: {cnn_num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Activations: Leaky ReLU (conv layers) + Tanh (FC layer)\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1326fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2294, Train Acc: 17.74%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.9526, Train Acc: 29.70%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 1.7244, Train Acc: 38.53%, Time: 0.9min\n",
      "Epoch [4/10], Loss: 1.5282, Train Acc: 45.03%, Time: 1.1min\n",
      "Epoch [5/10], Loss: 1.4241, Train Acc: 48.70%, Time: 1.4min\n",
      "Epoch [6/10], Loss: 1.3533, Train Acc: 51.56%, Time: 1.7min\n",
      "Epoch [7/10], Loss: 1.2895, Train Acc: 53.94%, Time: 2.0min\n",
      "Epoch [8/10], Loss: 1.2289, Train Acc: 56.32%, Time: 2.3min\n",
      "Epoch [9/10], Loss: 1.1743, Train Acc: 58.22%, Time: 2.5min\n",
      "Epoch [10/10], Loss: 1.1242, Train Acc: 60.30%, Time: 2.8min\n",
      "\n",
      "Total training time: 2.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training loop for CNN\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(cnn_num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in cnn_model.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = cnn_model.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        cnn_model.SGD_Optimizer(cnn_model.parameters(), cnn_learning_rate)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Print progress every epoch\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch+1}/{cnn_num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "print(f\"\\nTotal training time: {(time.time() - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51931b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "5-Layer CNN Test Accuracy: 58.86%\n",
      "==================================================\n",
      "\n",
      "üìä Comparison:\n",
      "  2-Layer Network: 48.04%\n",
      "  5-Layer CNN:     58.86%\n",
      "  Improvement:     10.82%\n",
      "\n",
      "‚úÖ SUCCESS: Deep network achieves >50% accuracy!\n",
      "‚úÖ This proves CIFAR-10 requires deep learning!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CNN on test set\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_test_accuracy = 100 * correct / total\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'5-Layer CNN Test Accuracy: {cnn_test_accuracy:.2f}%')\n",
    "print(f'{\"=\"*50}')\n",
    "\n",
    "# Compare with 2-layer network\n",
    "print(f'\\nüìä Comparison:')\n",
    "print(f'  2-Layer Network: 48.04%')\n",
    "print(f'  5-Layer CNN:     {cnn_test_accuracy:.2f}%')\n",
    "print(f'  Improvement:     {cnn_test_accuracy - 48.04:.2f}%')\n",
    "\n",
    "if cnn_test_accuracy > 50:\n",
    "    print(f'\\n‚úÖ SUCCESS: Deep network achieves >{50}% accuracy!')\n",
    "    print(f'‚úÖ This proves CIFAR-10 requires deep learning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Part 1 Complete!\n",
    "\n",
    "You now have:\n",
    "1. ‚úÖ **2-Layer Network** - 48.04% test accuracy (proves shallow networks struggle)\n",
    "2. ‚úÖ **5-Layer CNN** - Should get 50-60% test accuracy (proves depth helps)\n",
    "\n",
    "### üéØ Expected Training Time:\n",
    "- With batch_size=1 and 30 epochs: **~2-3 hours**\n",
    "- Each epoch processes 50,000 images individually\n",
    "\n",
    "### üí° Tips:\n",
    "- The training will take a while - be patient!\n",
    "- Loss should steadily decrease\n",
    "- Accuracy should improve over 2-layer baseline\n",
    "- You can reduce epochs to 20 if you're short on time\n",
    "\n",
    "### üìù Next Steps (Part 2):\n",
    "After this training completes, you'll:\n",
    "1. Test different activation functions (Leaky ReLU, Tanh)\n",
    "2. Implement mini-batch SGD (batch sizes: 16, 32, 64, 128)\n",
    "3. Add momentum to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b291a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2, Step 6: Mini-Batch SGD with Momentum\n",
    "\n",
    "**Previous Results:**\n",
    "- Vanilla SGD with batch_size=1: 65.24% test accuracy (29.9 min)\n",
    "- Mini-batch SGD with batch_size=32: 59.17% test accuracy (2.8 min)\n",
    "\n",
    "**Purpose:** Add momentum to accelerate learning and smooth optimization\n",
    "\n",
    "**Momentum Algorithm (Equation 8):**\n",
    "- **m‚ÇÅ = 0** (initialize momentum to zero)\n",
    "- **g·µ¢ = (1/b) Œ£ ‚àáŒ∏·µ¢L‚Çñ** (compute gradient)\n",
    "- **m·µ¢‚Çä‚ÇÅ = Œ±¬∑m·µ¢ + g·µ¢** (update momentum with Œ± = momentum coefficient)\n",
    "- **Œ∏·µ¢‚Çä‚ÇÅ = Œ∏ - Œ∑¬∑m·µ¢‚Çä‚ÇÅ** (update parameters using momentum)\n",
    "\n",
    "**How Momentum Helps:**\n",
    "- Accumulates a moving average of gradients\n",
    "- Dampens oscillations in directions with high curvature\n",
    "- Accelerates progress along consistent descent directions\n",
    "- Think: Rolling ball gaining speed downhill\n",
    "\n",
    "**Configuration:**\n",
    "- Architecture: 5-Layer CNN with Leaky ReLU + Tanh\n",
    "- Batch size: 32 (from previous experiment)\n",
    "- Learning rate: 0.005\n",
    "- **Momentum (Œ±): Testing 3 values: 0.7, 0.9, 0.95**\n",
    "- Epochs: 10\n",
    "\n",
    "**Expected:** Faster convergence, smoother training, potentially higher accuracy than vanilla SGD. Higher Œ± should provide more smoothing but may overshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7114d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing momentum with Œ± values: [0.7, 0.9, 0.95]\n",
      "Each training will take ~2.8 minutes\n",
      "Total expected time: ~8.5 minutes\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test multiple momentum values\n",
    "import time\n",
    "\n",
    "# Test different momentum coefficients (Œ±)\n",
    "alpha_values = [0.7, 0.9, 0.95]\n",
    "momentum_results = {}\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "\n",
    "print(f\"Testing momentum with Œ± values: {alpha_values}\")\n",
    "print(f\"Each training will take ~2.8 minutes\")\n",
    "print(f\"Total expected time: ~8.5 minutes\\n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302818af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.7\n",
      "======================================================================\n",
      "\n",
      "Model created with 288554 parameters\n",
      "Configuration: lr=0.005, Œ±=0.7, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 1.9255, Train Acc: 30.18%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.4703, Train Acc: 46.94%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 1.2743, Train Acc: 54.30%, Time: 0.9min\n",
      "Epoch [4/10], Loss: 1.1393, Train Acc: 59.51%, Time: 1.1min\n",
      "Epoch [5/10], Loss: 1.0317, Train Acc: 63.62%, Time: 1.4min\n",
      "Epoch [6/10], Loss: 0.9382, Train Acc: 67.12%, Time: 1.7min\n",
      "Epoch [7/10], Loss: 0.8623, Train Acc: 69.85%, Time: 2.0min\n",
      "Epoch [8/10], Loss: 0.7967, Train Acc: 72.03%, Time: 2.3min\n",
      "Epoch [9/10], Loss: 0.7341, Train Acc: 74.24%, Time: 2.5min\n",
      "Epoch [10/10], Loss: 0.6759, Train Acc: 76.33%, Time: 2.8min\n",
      "\n",
      "Training time: 2.8 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.7: 69.81%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.9\n",
      "======================================================================\n",
      "\n",
      "Model created with 288554 parameters\n",
      "Configuration: lr=0.005, Œ±=0.9, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 1.6520, Train Acc: 40.13%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.1518, Train Acc: 59.03%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 0.9542, Train Acc: 66.26%, Time: 0.8min\n",
      "Epoch [4/10], Loss: 0.8315, Train Acc: 70.77%, Time: 1.1min\n",
      "Epoch [5/10], Loss: 0.7370, Train Acc: 74.12%, Time: 1.4min\n",
      "Epoch [6/10], Loss: 0.6506, Train Acc: 77.20%, Time: 1.7min\n",
      "Epoch [7/10], Loss: 0.5736, Train Acc: 80.04%, Time: 2.0min\n",
      "Epoch [8/10], Loss: 0.5023, Train Acc: 82.47%, Time: 2.3min\n",
      "Epoch [9/10], Loss: 0.4339, Train Acc: 84.70%, Time: 2.6min\n",
      "Epoch [10/10], Loss: 0.3675, Train Acc: 87.23%, Time: 2.8min\n",
      "\n",
      "Training time: 2.8 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.9: 74.02%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîπ Training with Momentum Œ± = 0.95\n",
      "======================================================================\n",
      "\n",
      "Model created with 288554 parameters\n",
      "Configuration: lr=0.005, Œ±=0.95, epochs=10, batch_size=32\n",
      "\n",
      "Epoch [1/10], Loss: 1.5195, Train Acc: 44.63%, Time: 0.3min\n",
      "Epoch [2/10], Loss: 1.0458, Train Acc: 62.98%, Time: 0.6min\n",
      "Epoch [3/10], Loss: 0.8782, Train Acc: 69.29%, Time: 0.8min\n",
      "Epoch [4/10], Loss: 0.7646, Train Acc: 73.15%, Time: 1.1min\n",
      "Epoch [5/10], Loss: 0.6806, Train Acc: 76.07%, Time: 1.4min\n",
      "Epoch [6/10], Loss: 0.6103, Train Acc: 78.68%, Time: 1.7min\n",
      "Epoch [7/10], Loss: 0.5502, Train Acc: 80.69%, Time: 1.9min\n",
      "Epoch [8/10], Loss: 0.4926, Train Acc: 82.77%, Time: 2.2min\n",
      "Epoch [9/10], Loss: 0.4611, Train Acc: 83.59%, Time: 2.5min\n",
      "Epoch [10/10], Loss: 0.4220, Train Acc: 85.08%, Time: 2.8min\n",
      "\n",
      "Training time: 2.8 minutes\n",
      "\n",
      "‚úÖ Test Accuracy with Œ±=0.95: 71.18%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéØ MOMENTUM EXPERIMENTS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate for each momentum value\n",
    "for alpha in alpha_values:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîπ Training with Momentum Œ± = {alpha}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create fresh model for this alpha value\n",
    "    model_momentum = FiveLayerCNN().to(device)\n",
    "    print(f\"Model created with {sum(p.numel() for p in model_momentum.parameters())} parameters\")\n",
    "    print(f\"Configuration: lr={learning_rate}, Œ±={alpha}, epochs={num_epochs}, batch_size={batch_size}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model_momentum.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            for param in model_momentum.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model_momentum(images)\n",
    "            loss = model_momentum.cross_entropy_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights with momentum\n",
    "            model_momentum.SGD_Optimizer(model_momentum.parameters(), \n",
    "                                         learning_rate, \n",
    "                                         momentum=alpha)\n",
    "        \n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Print progress every epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining time: {training_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model_momentum.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_momentum(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Store results\n",
    "    momentum_results[alpha] = {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': avg_loss,\n",
    "        'training_time': training_time/60\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test Accuracy with Œ±={alpha}: {test_accuracy:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ MOMENTUM EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0b0776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä MOMENTUM EXPERIMENTS - COMPREHENSIVE RESULTS\n",
      "================================================================================\n",
      "Alpha (Œ±)    Test Acc     Train Acc    Loss         Time (min)  \n",
      "--------------------------------------------------------------------------------\n",
      "0.70         69.81        76.33        0.6759       2.8         \n",
      "0.90         74.02        87.23        0.3675       2.8         \n",
      "0.95         71.18        85.08        0.4220       2.8         \n",
      "================================================================================\n",
      "\n",
      "üèÜ Best Momentum Value: Œ± = 0.9 with 74.02% test accuracy\n",
      "\n",
      "================================================================================\n",
      "üìà COMPARISON WITH PREVIOUS EXPERIMENTS\n",
      "================================================================================\n",
      "Experiment                                    Test Accuracy   Training Time  \n",
      "--------------------------------------------------------------------------------\n",
      "Vanilla SGD (batch_size=1)                    65.24           29.9 min       \n",
      "Mini-batch SGD (batch_size=32, no momentum)   58.86           2.8 min        \n",
      "Mini-batch SGD with Momentum (Œ±=0.7)          69.81           2.8 min\n",
      "Mini-batch SGD with Momentum (Œ±=0.9)          74.02           2.8 min\n",
      "Mini-batch SGD with Momentum (Œ±=0.95)         71.18           2.8 min\n",
      "================================================================================\n",
      "\n",
      "üìù ANALYSIS:\n",
      "‚Ä¢ Best momentum value: Œ± = 0.9 with 74.02% test accuracy\n",
      "‚Ä¢ Improvement over no momentum: +15.16%\n",
      "‚Ä¢ Momentum helps by accumulating gradients in consistent directions\n",
      "‚Ä¢ Higher Œ± values preserve more history, lower values allow faster adaptation\n",
      "‚Ä¢ ‚úÖ Momentum successfully improved performance!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive results comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MOMENTUM EXPERIMENTS - COMPREHENSIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Alpha (Œ±)':<12} {'Test Acc':<12} {'Train Acc':<12} {'Loss':<12} {'Time (min)':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_alpha = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{alpha:<12.2f} {results['test_accuracy']:<12.2f} {results['train_accuracy']:<12.2f} \" \n",
    "          f\"{results['final_loss']:<12.4f} {results['training_time']:<12.1f}\")\n",
    "    \n",
    "    if results['test_accuracy'] > best_accuracy:\n",
    "        best_accuracy = results['test_accuracy']\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Momentum Value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\\n\")\n",
    "\n",
    "# Compare with baseline (mini-batch SGD without momentum)\n",
    "print(\"=\"*80)\n",
    "print(\"üìà COMPARISON WITH PREVIOUS EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Experiment':<45} {'Test Accuracy':<15} {'Training Time':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Vanilla SGD (batch_size=1)':<45} {65.24:<15.2f} {'29.9 min':<15}\")\n",
    "print(f\"{'Mini-batch SGD (batch_size=32, no momentum)':<45} {cnn_test_accuracy:<15.2f} {'2.8 min':<15}\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    results = momentum_results[alpha]\n",
    "    print(f\"{f'Mini-batch SGD with Momentum (Œ±={alpha})':<45} \"\n",
    "          f\"{results['test_accuracy']:<15.2f} {results['training_time']:.1f} min\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\nüìù ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Best momentum value: Œ± = {best_alpha} with {best_accuracy:.2f}% test accuracy\")\n",
    "print(f\"‚Ä¢ Improvement over no momentum: {best_accuracy - cnn_test_accuracy:+.2f}%\")\n",
    "print(f\"‚Ä¢ Momentum helps by accumulating gradients in consistent directions\")\n",
    "print(f\"‚Ä¢ Higher Œ± values preserve more history, lower values allow faster adaptation\")\n",
    "if best_accuracy > cnn_test_accuracy:\n",
    "    print(f\"‚Ä¢ ‚úÖ Momentum successfully improved performance!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2037e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: SKIP CONNECTIONS (Residual Learning)\n",
    "\n",
    "## Motivation\n",
    "As networks become deeper, training becomes more difficult due to:\n",
    "1. **Vanishing gradients**: Gradients become exponentially small in early layers\n",
    "2. **Degradation problem**: Deep networks can perform worse than shallow ones (not due to overfitting)\n",
    "3. **Optimization difficulty**: Deeper networks are harder to optimize\n",
    "\n",
    "**Skip connections (Residual connections)** solve this by:\n",
    "- Creating shortcuts that bypass layers\n",
    "- Allowing gradients to flow directly backward\n",
    "- Learning residual functions F(x) = H(x) - x instead of H(x) directly\n",
    "- Enabling very deep networks (50, 100, even 1000+ layers)\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "We will conduct **THREE experiments** with a 15-layer deep CNN:\n",
    "1. **Extended model WITHOUT skip connections** (plain deep network) - Baseline to observe degradation\n",
    "2. **Extended model WITH skip connections - Configuration 1** - First skip connection strategy (3 skips)\n",
    "3. **Extended model WITH skip connections - Configuration 2** - Second skip connection strategy (3 skips)\n",
    "\n",
    "**Assignment Requirement:** Add 10 layers to our 5-layer CNN (making it 15 layers total), then test with two different skip connection configurations, each using 3 skip connections.\n",
    "\n",
    "**Hypothesis:** \n",
    "- The plain 15-layer network may suffer from degradation (perform worse than 5-layer)\n",
    "- Skip connections will enable effective training and improve gradient flow\n",
    "- Different skip configurations may yield different performance\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "**15-Layer CNN Structure (Extended Model):**\n",
    "- **Block 1:** Conv 3‚Üí16, then 3√ó Conv 16‚Üí16 (same shape, ideal for skip connections)\n",
    "- **Block 2:** Conv 16‚Üí32, then 3√ó Conv 32‚Üí32 (same shape, ideal for skip connections)\n",
    "- **Block 3:** Conv 32‚Üí64, then 3√ó Conv 64‚Üí64 (same shape, ideal for skip connections)\n",
    "- **Block 4:** FC 1024‚Üí256, FC 256‚Üí256, FC 256‚Üí10\n",
    "- **Total:** 15 parameterized layers (12 conv + 3 FC) = 5-layer baseline + 10 additional layers\n",
    "\n",
    "### Skip Connection Configurations\n",
    "\n",
    "**Configuration 1: Short Skips (Length 1)** - ResNet-style, frequent gradient shortcuts\n",
    "- Skip 1: conv1 output ‚Üí conv2 output (y = x ‚äï f(x), skips 1 layer)\n",
    "- Skip 2: conv5 output ‚Üí conv6 output (y = x ‚äï f(x), skips 1 layer) \n",
    "- Skip 3: conv9 output ‚Üí conv10 output (y = x ‚äï f(x), skips 1 layer)\n",
    "\n",
    "**Configuration 2: Longer Skips (Length 2-3)** - Deeper gradient shortcuts\n",
    "- Skip 1: conv1 output ‚Üí conv3 output (y = x ‚äï g(f(x)), skips 2 layers)\n",
    "- Skip 2: conv5 output ‚Üí conv8 output (y = x ‚äï h(g(f(x))), skips 3 layers)\n",
    "- Skip 3: conv9 output ‚Üí conv12 output (y = x ‚äï h(g(f(x))), skips 3 layers)\n",
    "\n",
    "**Training Configuration:**\n",
    "- Use best hyperparameters from Part 2: batch_size=32, momentum=0.9, lr=0.005\n",
    "- Train for 10 epochs for all three models\n",
    "- Record average L1-norm of gradients during first epoch for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471732e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-Layer Deep CNN WITHOUT Skip Connections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_NoSkip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_NoSkip, self).__init__()\n",
    "        \n",
    "        # Block 1: 3‚Üí16‚Üí16‚Üí16‚Üí16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 1024‚Üí256‚Üí256‚Üí10\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv5(x))\n",
    "        x = self.leaky_relu(self.conv6(x))\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3: 4 conv layers\n",
    "        x = self.leaky_relu(self.conv9(x))\n",
    "        x = self.leaky_relu(self.conv10(x))\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test the model\n",
    "model_deep15_no_skip = DeepCNN15_NoSkip().to(device)\n",
    "total_params = sum(p.numel() for p in model_deep15_no_skip.parameters())\n",
    "print(\"=\"*70)\n",
    "print(\"15-LAYER DEEP CNN (NO SKIP CONNECTIONS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  Block 1: Conv 3‚Üí16, 3√ó Conv 16‚Üí16, MaxPool\")\n",
    "print(\"  Block 2: Conv 16‚Üí32, 3√ó Conv 32‚Üí32, MaxPool\")\n",
    "print(\"  Block 3: Conv 32‚Üí64, 3√ó Conv 64‚Üí64, MaxPool\")\n",
    "print(\"  Block 4: FC 1024‚Üí256, FC 256‚Üí256, FC 256‚Üí10\")\n",
    "print(\"\\nTotal: 15 parameterized layers (12 conv + 3 FC)\")\n",
    "print(\"Activation: Leaky ReLU (conv), Tanh (FC)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 15-layer CNN WITHOUT skip connections\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING 15-LAYER CNN (NO SKIP CONNECTIONS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\")\n",
    "print(\"Recording gradient L1-norms during first epoch...\\n\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum_alpha = 0.9\n",
    "num_epochs = 10\n",
    "\n",
    "gradient_norms_no_skip = []  # Store gradient norms for first epoch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_deep15_no_skip.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        for param in model_deep15_no_skip.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_deep15_no_skip(images)\n",
    "        loss = model_deep15_no_skip.cross_entropy_loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Record gradient L1-norms during first epoch\n",
    "        if epoch == 0:\n",
    "            batch_grad_norm = 0\n",
    "            for param in model_deep15_no_skip.parameters():\n",
    "                if param.grad is not None:\n",
    "                    batch_grad_norm += torch.sum(torch.abs(param.grad)).item()\n",
    "            gradient_norms_no_skip.append(batch_grad_norm)\n",
    "        \n",
    "        # Update weights with momentum\n",
    "        model_deep15_no_skip.SGD_Optimizer(model_deep15_no_skip.parameters(), \n",
    "                                            learning_rate, \n",
    "                                            momentum=momentum_alpha)\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "\n",
    "training_time_no_skip = time.time() - start_time\n",
    "\n",
    "# Calculate average gradient norm from first epoch\n",
    "avg_grad_norm_no_skip = sum(gradient_norms_no_skip) / len(gradient_norms_no_skip)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Training time: {training_time_no_skip/60:.1f} minutes\")\n",
    "print(f\"Average gradient L1-norm (epoch 1): {avg_grad_norm_no_skip:.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 15-layer CNN WITHOUT skip connections\n",
    "model_deep15_no_skip.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_deep15_no_skip(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc_no_skip = 100 * correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"15-LAYER CNN (NO SKIP) - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_acc_no_skip:.2f}%\")\n",
    "print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Generalization Gap: {train_acc - test_acc_no_skip:.2f}%\")\n",
    "print(f\"Average Gradient Norm (epoch 1): {avg_grad_norm_no_skip:.2f}\")\n",
    "print(\"\\nüìä Comparison with 5-layer CNN:\")\n",
    "print(f\"  5-layer with momentum:  {best_accuracy:.2f}%\")\n",
    "print(f\"  15-layer no skip:       {test_acc_no_skip:.2f}%\")\n",
    "print(f\"  Difference:             {test_acc_no_skip - best_accuracy:+.2f}%\")\n",
    "\n",
    "if test_acc_no_skip < best_accuracy:\n",
    "    print(\"\\n‚ö†Ô∏è  DEGRADATION OBSERVED: Deeper network performs worse!\")\n",
    "    print(\"    This demonstrates the degradation problem in plain deep networks.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Deeper network improved performance!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-Layer Deep CNN WITH Skip Connections - CONFIGURATION 1 (Short Skips, Length 1)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_SkipConfig1(nn.Module):\n",
    "    \"\"\"\n",
    "    Configuration 1: Short skip connections (length 1)\n",
    "    - Skip 1: conv1 ‚Üí conv2 (skips 1 layer)\n",
    "    - Skip 2: conv5 ‚Üí conv6 (skips 1 layer)\n",
    "    - Skip 3: conv9 ‚Üí conv10 (skips 1 layer)\n",
    "    Total: 3 skip connections (ResNet-style)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_SkipConfig1, self).__init__()\n",
    "        \n",
    "        # Block 1: 3‚Üí16‚Üí16‚Üí16‚Üí16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 1024‚Üí256‚Üí256‚Üí10\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1 with SHORT skip (length 1): conv1 ‚Üí conv2\n",
    "        identity1 = self.leaky_relu(self.conv1(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv2(identity1))\n",
    "        x = x + identity1  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2 with SHORT skip (length 1): conv5 ‚Üí conv6\n",
    "        identity2 = self.leaky_relu(self.conv5(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv6(identity2))\n",
    "        x = x + identity2  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3 with SHORT skip (length 1): conv9 ‚Üí conv10\n",
    "        identity3 = self.leaky_relu(self.conv9(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv10(identity3))\n",
    "        x = x + identity3  # Skip connection: y = x ‚äï f(x)\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers (no skip connections in this configuration)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test Config 1 model\n",
    "model_skip_config1 = DeepCNN15_SkipConfig1().to(device)\n",
    "total_params_c1 = sum(p.numel() for p in model_skip_config1.parameters())\n",
    "print(\"=\"*70)\n",
    "print(\"15-LAYER CNN - SKIP CONFIGURATION 1 (SHORT SKIPS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params_c1:,}\")\n",
    "print(\"\\nSkip Connection Configuration 1:\")\n",
    "print(\"  Skip 1: conv1 output ‚Üí conv2 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"  Skip 2: conv5 output ‚Üí conv6 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"  Skip 3: conv9 output ‚Üí conv10 output (length 1, y = x ‚äï f(x))\")\n",
    "print(\"\\nTotal: 3 skip connections (all length 1)\")\n",
    "print(\"Strategy: Frequent, short gradientpathways (ResNet-style)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15-Layer Deep CNN WITH Skip Connections - CONFIGURATION 2 (Longer Skips, Length 2-3)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepCNN15_SkipConfig2(nn.Module):\n",
    "    \"\"\"\n",
    "    Configuration 2: Longer skip connections (length 2-3)\n",
    "    - Skip 1: conv1 ‚Üí conv3 (skips 2 layers)\n",
    "    - Skip 2: conv5 ‚Üí conv8 (skips 3 layers)\n",
    "    - Skip 3: conv9 ‚Üí conv12 (skips 3 layers)\n",
    "    Total: 3 skip connections\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeepCNN15_SkipConfig2, self).__init__()\n",
    "        \n",
    "        # Block 1: 3‚Üí16‚Üí16‚Üí16‚Üí16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16‚Üí32‚Üí32‚Üí32‚Üí32\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32‚Üí64‚Üí64‚Üí64‚Üí64\n",
    "        self.conv9 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers: 1024‚Üí256‚Üí256‚Üí10\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Custom Leaky ReLU activation\"\"\"\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Custom Tanh activation\"\"\"\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1 with LONGER skip (length 2): conv1 ‚Üí conv3\n",
    "        identity1 = self.leaky_relu(self.conv1(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv2(identity1))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = x + identity1  # Skip connection: y = x ‚äï g(f(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Block 2 with LONGER skip (length 3): conv5 ‚Üí conv8\n",
    "        identity2 = self.leaky_relu(self.conv5(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv6(identity2))\n",
    "        x = self.leaky_relu(self.conv7(x))\n",
    "        x = self.leaky_relu(self.conv8(x))\n",
    "        x = x + identity2  # Skip connection: y = x ‚äï h(g(f(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Block 3 with LONGER skip (length 3): conv9 ‚Üí conv12\n",
    "        identity3 = self.leaky_relu(self.conv9(x))  # Save for skip\n",
    "        x = self.leaky_relu(self.conv10(identity3))\n",
    "        x = self.leaky_relu(self.conv11(x))\n",
    "        x = self.leaky_relu(self.conv12(x))\n",
    "        x = x + identity3  # Skip connection: y = x ‚äï h(g(f(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers (no skip connections in this configuration)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cross_entropy_loss(self, predictions, targets):\n",
    "        \"\"\"Manual cross-entropy loss implementation\"\"\"\n",
    "        predictions = predictions - predictions.max(dim=1, keepdim=True)[0]\n",
    "        exp_pred = torch.exp(predictions)\n",
    "        softmax = exp_pred / exp_pred.sum(dim=1, keepdim=True)\n",
    "        log_softmax = torch.log(softmax + 1e-8)\n",
    "        loss = -log_softmax[range(len(targets)), targets].mean()\n",
    "        return loss\n",
    "    \n",
    "    def SGD_Optimizer(self, params, lr, momentum=0.0):\n",
    "        \"\"\"Manual SGD optimizer with momentum\"\"\"\n",
    "        if not hasattr(self, 'momentum_buffer'):\n",
    "            self.momentum_buffer = {}\n",
    "        \n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                \n",
    "                if param_id not in self.momentum_buffer:\n",
    "                    self.momentum_buffer[param_id] = torch.zeros_like(param.data)\n",
    "                \n",
    "                # m_{i+1} = Œ± * m_i + g_i\n",
    "                self.momentum_buffer[param_id] = (\n",
    "                    momentum * self.momentum_buffer[param_id] + param.grad.data\n",
    "                )\n",
    "                \n",
    "                # Œ∏_{i+1} = Œ∏_i - Œ∑ * m_{i+1}\n",
    "                param.data = param.data - lr * self.momentum_buffer[param_id]\n",
    "\n",
    "# Create and test Config 2 model\n",
    "model_skip_config2 = DeepCNN15_SkipConfig2().to(device)\n",
    "total_params_c2 = sum(p.numel() for p in model_skip_config2.parameters())\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"15-LAYER CNN - SKIP CONFIGURATION 2 (LONGER SKIPS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters: {total_params_c2:,}\")\n",
    "print(\"\\nSkip Connection Configuration 2:\")\n",
    "print(\"  Skip 1: conv1 output ‚Üí conv3 output (length 2, y = x ‚äï g(f(x)))\")\n",
    "print(\"  Skip 2: conv5 output ‚Üí conv8 output (length 3, y = x ‚äï h(g(f(x))))\")\n",
    "print(\"  Skip 3: conv9 output ‚Üí conv12 output (length 3, y = x ‚äï h(g(f(x))))\")\n",
    "print(\"\\nTotal: 3 skip connections (1√ó length 2, 2√ó length 3)\")\n",
    "print(\"Strategy: Sparser, longer gradient pathways\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all THREE models: No skip, Config 1, Config 2\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: TRAINING THREE 15-LAYER MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Configuration: lr=0.005, momentum=0.9, batch_size=32, epochs=10\\n\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum_alpha = 0.9\n",
    "num_epochs = 10\n",
    "\n",
    "# Dictionary to store results for all 3 models\n",
    "deep_results = {\n",
    "    'no_skip': {},\n",
    "    'config1': {},\n",
    "    'config2': {}\n",
    "}\n",
    "\n",
    "# Model configurations\n",
    "models_to_train = [\n",
    "    ('no_skip', model_deep15_no_skip, \"NO SKIP CONNECTIONS (Baseline)\"),\n",
    "    ('config1', model_skip_config1, \"SKIP CONFIG 1 (Short Skips)\"),\n",
    "    ('config2', model_skip_config2, \"SKIP CONFIG 2 (Longer Skips)\")\n",
    "]\n",
    "\n",
    "for model_key, model, model_name in models_to_train:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üîπ TRAINING: {model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gradient_norms = []  # Store gradient norms for first epoch\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = model.cross_entropy_loss(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Record gradient L1-norms during first epoch\n",
    "            if epoch == 0:\n",
    "                batch_grad_norm = 0\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        batch_grad_norm += torch.sum(torch.abs(param.grad)).item()\n",
    "                gradient_norms.append(batch_grad_norm)\n",
    "            \n",
    "            # Update weights with momentum\n",
    "            model.SGD_Optimizer(model.parameters(), learning_rate, momentum=momentum_alpha)\n",
    "        \n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Time: {elapsed_time/60:.1f}min')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    avg_grad_norm = sum(gradient_norms) / len(gradient_norms)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    \n",
    "    # Store results\n",
    "    deep_results[model_key] = {\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': avg_loss,\n",
    "        'training_time': training_time/60,\n",
    "        'avg_grad_norm': avg_grad_norm\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Average Gradient L1-norm (epoch 1): {avg_grad_norm:.2f}\")\n",
    "    print(f\"Training time: {training_time/60:.1f} minutes\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ ALL THREE MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Comprehensive Comparison and Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: COMPREHENSIVE RESULTS - SKIP CONNECTIONS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract results for easier access\n",
    "no_skip = deep_results['no_skip']\n",
    "config1 = deep_results['config1']\n",
    "config2 = deep_results['config2']\n",
    "\n",
    "print(\"\\nüìä MODEL COMPARISON TABLE:\")\n",
    "print(f\"{'Model':<45} {'Test Acc':<12} {'Train Acc':<12} {'Grad Norm':<15} {'Time':<10}\")\n",
    "print(\"-\"*95)\n",
    "print(f\"{'5-layer CNN (Part 2 baseline)':<45} {best_accuracy:<12.2f} {'-':<12} {'-':<15} {'2.8 min':<10}\")\n",
    "print(f\"{'15-layer WITHOUT skip connections':<45} {no_skip['test_accuracy']:<12.2f} {no_skip['train_accuracy']:<12.2f} {no_skip['avg_grad_norm']:<15.2f} {no_skip['training_time']:.1f} min\")\n",
    "print(f\"{'15-layer WITH skip - Config 1 (short)':<45} {config1['test_accuracy']:<12.2f} {config1['train_accuracy']:<12.2f} {config1['avg_grad_norm']:<15.2f} {config1['training_time']:.1f} min\")\n",
    "print(f\"{'15-layer WITH skip - Config 2 (longer)':<45} {config2['test_accuracy']:<12.2f} {config2['train_accuracy']:<12.2f} {config2['avg_grad_norm']:<15.2f} {config2['training_time']:.1f} min\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "print(\"\\nüìà GRADIENT FLOW ANALYSIS:\")\n",
    "print(f\"  No skip connections:       {no_skip['avg_grad_norm']:.2f}\")\n",
    "print(f\"  Config 1 (short skips):    {config1['avg_grad_norm']:.2f}  (ratio: {config1['avg_grad_norm']/no_skip['avg_grad_norm']:.2f}x)\")\n",
    "print(f\"  Config 2 (longer skips):   {config2['avg_grad_norm']:.2f}  (ratio: {config2['avg_grad_norm']/no_skip['avg_grad_norm']:.2f}x)\")\n",
    "\n",
    "if config1['avg_grad_norm'] > no_skip['avg_grad_norm']:\n",
    "    improvement = ((config1['avg_grad_norm']/no_skip['avg_grad_norm']) - 1) * 100\n",
    "    print(f\"\\n  ‚úÖ Config 1 increased gradient magnitudes by {improvement:.1f}%\")\n",
    "if config2['avg_grad_norm'] > no_skip['avg_grad_norm']:\n",
    "    improvement = ((config2['avg_grad_norm']/no_skip['avg_grad_norm']) - 1) * 100\n",
    "    print(f\"  ‚úÖ Config 2 increased gradient magnitudes by {improvement:.1f}%\")\n",
    "print(f\"  ‚Üí Skip connections combat vanishing gradients!\")\n",
    "\n",
    "print(\"\\nüìä ACCURACY ANALYSIS:\")\n",
    "print(f\"  Extended model (no skip) vs 5-layer:  {no_skip['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "print(f\"  Config 1 vs No Skip:                   {config1['test_accuracy'] - no_skip['test_accuracy']:+.2f}% improvement\")\n",
    "print(f\"  Config 2 vs No Skip:                   {config2['test_accuracy'] - no_skip['test_accuracy']:+.2f}% improvement\")\n",
    "print(f\"  Config 1 vs 5-layer baseline:          {config1['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "print(f\"  Config 2 vs 5-layer baseline:          {config2['test_accuracy'] - best_accuracy:+.2f}%\")\n",
    "\n",
    "# Determine best skip configuration\n",
    "best_skip_model = 'config1' if config1['test_accuracy'] >= config2['test_accuracy'] else 'config2'\n",
    "best_skip_name = \"Config 1 (short skips)\" if best_skip_model == 'config1' else \"Config 2 (longer skips)\"\n",
    "best_skip_acc = deep_results[best_skip_model]['test_accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ BEST SKIP CONFIGURATION: {best_skip_name} with {best_skip_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nüîç KEY FINDINGS:\")\n",
    "\n",
    "# Finding 1: Degradation problem\n",
    "if no_skip['test_accuracy'] < best_accuracy:\n",
    "    print(f\"  1. ‚ö†Ô∏è  DEGRADATION PROBLEM OBSERVED:\")\n",
    "    print(f\"      ‚Ä¢ 15-layer without skip: {no_skip['test_accuracy']:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:      {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ Difference:            {no_skip['test_accuracy'] - best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Üí Adding layers WITHOUT skip connections HURTS performance!\")\n",
    "    print(f\"      ‚Üí This is NOT overfitting - it's an optimization failure\")\n",
    "else:\n",
    "    print(f\"  1. Extended model (no skip) achieved {no_skip['test_accuracy']:.2f}%\")\n",
    "    if no_skip['test_accuracy'] > best_accuracy:\n",
    "        print(f\"      ‚Üí Deeper network improved over 5-layer!\")\n",
    "    else:\n",
    "        print(f\"      ‚Üí Similar performance to 5-layer baseline\")\n",
    "\n",
    "# Finding 2: Skip connections solve degradation\n",
    "if config1['test_accuracy'] > no_skip['test_accuracy'] or config2['test_accuracy'] > no_skip['test_accuracy']:\n",
    "    print(f\"  2. ‚úÖ SKIP CONNECTIONS SOLVE DEGRADATION:\")\n",
    "    print(f\"      ‚Ä¢ Config 1 (short):  {config1['test_accuracy']:.2f}% ({config1['test_accuracy'] - no_skip['test_accuracy']:+.2f}% vs no skip)\")\n",
    "    print(f\"      ‚Ä¢ Config 2 (longer): {config2['test_accuracy']:.2f}% ({config2['test_accuracy'] - no_skip['test_accuracy']:+.2f}% vs no skip)\")\n",
    "    print(f\"      ‚Üí Skip connections enable training of deep networks!\")\n",
    "\n",
    "# Finding 3: Gradient flow improvement\n",
    "grad_improve_c1 = (config1['avg_grad_norm'] / no_skip['avg_grad_norm'] - 1) * 100\n",
    "grad_improve_c2 = (config2['avg_grad_norm'] / no_skip['avg_grad_norm'] - 1) * 100\n",
    "print(f\"  3. üìä GRADIENT FLOW IMPROVEMENT:\")\n",
    "print(f\"      ‚Ä¢ Config 1: {grad_improve_c1:+.1f}% stronger gradients\")\n",
    "print(f\"      ‚Ä¢ Config 2: {grad_improve_c2:+.1f}% stronger gradients\")\n",
    "print(f\"      ‚Üí Skip connections provide gradient highways to early layers\")\n",
    "\n",
    "# Finding 4: Configuration comparison\n",
    "print(f\"  4. üî¨ SKIP CONFIGURATION COMPARISON:\")\n",
    "if config1['test_accuracy'] > config2['test_accuracy']:\n",
    "    diff = config1['test_accuracy'] - config2['test_accuracy']\n",
    "    print(f\"      ‚Ä¢ Config 1 (short/frequent) outperformed Config 2 by {diff:.2f}%\")\n",
    "    print(f\"      ‚Üí Frequent short skips (ResNet-style) work better for this architecture\")\n",
    "    print(f\"      ‚Üí More gradient pathways = better optimization\")\n",
    "elif config2['test_accuracy'] > config1['test_accuracy']:\n",
    "    diff = config2['test_accuracy'] - config1['test_accuracy']\n",
    "    print(f\"      ‚Ä¢ Config 2 (longer/sparser) outperformed Config 1 by {diff:.2f}%\")\n",
    "    print(f\"      ‚Üí Longer skips provide stronger gradient flow\")\n",
    "    print(f\"      ‚Üí Deeper shortcuts may be more effective\")\n",
    "else:\n",
    "    print(f\"      ‚Ä¢ Both configurations achieved similar performance ({config1['test_accuracy']:.2f}%)\")\n",
    "    print(f\"      ‚Üí Both skip strategies are effective for gradient flow\")\n",
    "\n",
    "# Check if skip connections beat 5-layer baseline\n",
    "if best_skip_acc > best_accuracy:\n",
    "    improvement = best_skip_acc - best_accuracy\n",
    "    print(f\"  5. ‚úÖ DEEP NETWORK SUCCESS:\")\n",
    "    print(f\"      ‚Ä¢ Best 15-layer (with skip): {best_skip_acc:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:          {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ Improvement:               +{improvement:.2f}%\")\n",
    "    print(f\"      ‚Üí Skip connections enabled a deeper network to outperform shallow!\")\n",
    "elif best_skip_acc >= best_accuracy - 2:\n",
    "    print(f\"  5. ‚úì COMPETITIVE PERFORMANCE:\")\n",
    "    print(f\"      ‚Ä¢ Best 15-layer (with skip): {best_skip_acc:.2f}%\")\n",
    "    print(f\"      ‚Ä¢ 5-layer baseline:          {best_accuracy:.2f}%\")\n",
    "    print(f\"      ‚Üí Skip connections enabled training a 3√ó deeper network\")\n",
    "else:\n",
    "    print(f\"  5. ‚ö†Ô∏è  Still below 5-layer baseline, but skip connections helped significantly\")\n",
    "\n",
    "print(\"\\nüí° CONCLUSIONS:\")\n",
    "print(\"  ‚Ä¢ Skip connections are ESSENTIAL for training deep neural networks\")\n",
    "print(\"  ‚Ä¢ They solve the degradation problem by providing gradient highways\")\n",
    "print(\"  ‚Ä¢ Stronger gradient flow ‚Üí Better optimization ‚Üí Higher accuracy\")\n",
    "print(f\"  ‚Ä¢ Best approach: {best_skip_name}\")\n",
    "print(f\"  ‚Ä¢ Best 15-layer model: {best_skip_acc:.2f}% test accuracy\")\n",
    "print(\"  ‚Ä¢ Without skip connections, adding layers can hurt performance!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
